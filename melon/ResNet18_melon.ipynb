{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1UXBZ2kzoX1",
        "outputId": "3402acab-8cd2-46e0-c4e4-5e8b77a81df1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pynvml\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynvml\n",
            "Successfully installed pynvml-11.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iR74754xWIR3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pynvml import *\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KutboPCzza4_"
      },
      "outputs": [],
      "source": [
        "def print_gpu_utilization():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.cuda.current_device()  # 현재 GPU 디바이스 정보\n",
        "        allocated_memory = torch.cuda.memory_allocated(device) / 1024**3  # 메모리 사용량 (GB)\n",
        "        reserved_memory = torch.cuda.memory_reserved(device) / 1024**3  # 예약된 메모리 (GB)\n",
        "        print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
        "        print(f\"Reserved Memory: {reserved_memory:.2f} GB\")\n",
        "    else:\n",
        "        print(\"No GPU available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_8xbvf80zvjG"
      },
      "outputs": [],
      "source": [
        "def print_summary(result):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ntBtU40OWIR5"
      },
      "outputs": [],
      "source": [
        "size = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NkTBa1iCWIR5"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size, scale=(0.5, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68SaArxkWIR5",
        "outputId": "403a3755-d094-425d-9b2f-661873f9ca4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-10\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HISYmEIlWIR6"
      },
      "outputs": [],
      "source": [
        "VALID_RATIO = 0.7\n",
        "n_train_examples = int(len(trainset) * VALID_RATIO)\n",
        "n_valid_examples = len(trainset) - n_train_examples\n",
        "\n",
        "train_data, valid_data = data.random_split(trainset, [n_train_examples, n_valid_examples])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nKkwwymDWIR6"
      },
      "outputs": [],
      "source": [
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transform = test_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kZE0KyvWIR6",
        "outputId": "3cbd5996-b958-415a-a873-1241ddb7272e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35000, 15000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(train_data), len(valid_data), len(testset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NgFbnxO2WIR7"
      },
      "outputs": [],
      "source": [
        "sample_fraction = 0.2\n",
        "\n",
        "# 무작위 인덱스 생성\n",
        "train_indices = torch.randperm(len(trainset))[:int(len(trainset) * sample_fraction)]\n",
        "valid_indices = torch.randperm(len(valid_data))[:int(len(valid_data) * sample_fraction)]\n",
        "test_indices = torch.randperm(len(testset))[:int(len(testset) * sample_fraction)]\n",
        "\n",
        "# 서브셋 생성\n",
        "train_subset = Subset(trainset, train_indices)\n",
        "valid_subset = Subset(valid_data, valid_indices)\n",
        "test_subset = Subset(testset, test_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UMCcWRjWIR7",
        "outputId": "b75844fb-c6f1-4218-ab9b-2a6466352ffa"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 3000, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(train_subset), len(valid_subset), len(test_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KIuqCxeGWIR7"
      },
      "outputs": [],
      "source": [
        "train_iterator = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "valid_iterator = DataLoader(valid_subset, batch_size=batch_size, shuffle=False)\n",
        "test_iterator = DataLoader(test_subset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6mVcfuK7WIR7"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample = False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if downsample:\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            downsample = None\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        i = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            i = self.downsample(i)\n",
        "\n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ijsu-aH8WIR7"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, config, output_dim, zero_init_residual = False):\n",
        "        super().__init__()\n",
        "\n",
        "        block, n_blocks, channels = config\n",
        "        self.in_channels = channels[0]\n",
        "        assert len(n_blocks) == len(channels) == 4\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
        "        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride=2)\n",
        "        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride=2)\n",
        "        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(self.in_channels, output_dim)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "                #elif isinstance(m, Bottleneck):\n",
        "                    #nn.init.constant_(m.bn3.weight, 0)\n",
        "\n",
        "    def get_resnet_layer(self, block, n_blocks, channels, stride=1):\n",
        "        layers = []\n",
        "        if self.in_channels != block.expansion * channels:\n",
        "            downsample = True\n",
        "        else:\n",
        "            downsample = False\n",
        "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "        for i in range(1, n_blocks):\n",
        "            layers.append(block(block.expansion * channels, channels))\n",
        "\n",
        "        self.in_channels = block.expansion * channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        h = x.view(x.shape[0], -1)\n",
        "        x = self.fc(h)\n",
        "        return x, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fZ2g6Vv5WIR8"
      },
      "outputs": [],
      "source": [
        "ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JyqkNDbOWIR8"
      },
      "outputs": [],
      "source": [
        "resnet18_config = ResNetConfig(block = BasicBlock, n_blocks = [2, 2, 2, 2], channels = [64, 128, 256, 512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvxhhdRPWIR8",
        "outputId": "d05eb8da-66e1-4532-a7d5-604adf757dff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 224MB/s]\n"
          ]
        }
      ],
      "source": [
        "pretrained_model = models.resnet18(pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApjRni8tWIR8",
        "outputId": "64a10b63-6816-4f5b-d661-e238286cd9fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(pretrained_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CGaz_A9yWIR8"
      },
      "outputs": [],
      "source": [
        "model = ResNet(resnet18_config, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf05XuEVY50-",
        "outputId": "fd7726b3-2f31-4348-8321-033858789c3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "I-fZ50Ozf97u"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0fCSieg3WIR8"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "pretrained_model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "GwVmEYKxWIR8"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim=True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qJaIA_0BTc_t"
      },
      "outputs": [],
      "source": [
        "pattern = re.compile(r'key=(?P<key>\\S+)\\s+'\n",
        "                     r'self_cpu_time=(?P<self_cpu_time>\\S+)\\s+'\n",
        "                     r'cpu_time=(?P<cpu_time>\\S+)\\s+'\n",
        "                     r'self_cuda_time=(?P<self_cuda_time>\\S+)\\s+'\n",
        "                     r'cuda_time=(?P<cuda_time>\\S+)\\s+'\n",
        "                     r'input_shapes=(?P<input_shapes>\\S*)\\s*'\n",
        "                     r'cpu_memory_usage=(?P<cpu_memory_usage>\\S*)\\s*'\n",
        "                     r'cuda_memory_usage=(?P<cuda_memory_usage>\\S*)')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducing the Melon"
      ],
      "metadata": {
        "id": "9irrA1EUa8p6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LifetimeAwareMemoryPool:\n",
        "    def __init__(self, memory_budget):\n",
        "        self.memory_budget = memory_budget\n",
        "        self.allocated_memory = 0\n",
        "        self.memory_blocks = []  # (start_addr, size, tensor_id, lifetime)\n",
        "        self.tensor_map = {}     # tensor_id -> memory_block_index\n",
        "\n",
        "    def allocate(self, tensor_id, size, lifetime):\n",
        "        \"\"\"텐서의 수명을 고려한 메모리 할당\"\"\"\n",
        "        # 이미 할당된 텐서인 경우\n",
        "        if tensor_id in self.tensor_map:\n",
        "            return self.tensor_map[tensor_id]\n",
        "\n",
        "        # 가용 메모리 공간 찾기\n",
        "        best_addr = self._find_best_fit(size, lifetime)\n",
        "\n",
        "        # 메모리 부족한 경우\n",
        "        if best_addr is None:\n",
        "            self._compact()  # 메모리 조각 모음\n",
        "            best_addr = self._find_best_fit(size, lifetime)\n",
        "            if best_addr is None:\n",
        "                raise MemoryError(\"Not enough memory\")\n",
        "\n",
        "        # 새로운 메모리 블록 할당\n",
        "        block_index = len(self.memory_blocks)\n",
        "        self.memory_blocks.append((best_addr, size, tensor_id, lifetime))\n",
        "        self.tensor_map[tensor_id] = block_index\n",
        "        self.allocated_memory += size\n",
        "\n",
        "        return best_addr\n",
        "\n",
        "    def free(self, tensor_id):\n",
        "        \"\"\"텐서 메모리 해제\"\"\"\n",
        "        if tensor_id in self.tensor_map:\n",
        "            block_index = self.tensor_map[tensor_id]\n",
        "            _, size, _, _ = self.memory_blocks[block_index]\n",
        "            self.allocated_memory -= size\n",
        "            del self.tensor_map[tensor_id]\n",
        "            self.memory_blocks[block_index] = None\n",
        "\n",
        "    def _find_best_fit(self, size, lifetime):\n",
        "        \"\"\"최적의 메모리 위치 찾기\"\"\"\n",
        "        if self.allocated_memory + size > self.memory_budget:\n",
        "            return None\n",
        "\n",
        "        # 긴 수명의 텐서는 낮은 주소에 할당\n",
        "        available_addr = 0\n",
        "        for block in self.memory_blocks:\n",
        "            if block is None:\n",
        "                continue\n",
        "            block_addr, block_size, _, block_lifetime = block\n",
        "\n",
        "            # 수명이 겹치지 않는 경우 해당 공간 재사용 가능\n",
        "            if not self._lifetimes_overlap(lifetime, block_lifetime):\n",
        "                if block_addr - available_addr >= size:\n",
        "                    return available_addr\n",
        "            available_addr = max(available_addr, block_addr + block_size)\n",
        "\n",
        "        # 새로운 메모리 영역 할당\n",
        "        if self.allocated_memory + size <= self.memory_budget:\n",
        "            return available_addr\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _compact(self):\n",
        "        \"\"\"메모리 조각 모음\"\"\"\n",
        "        # 유효한 블록만 필터링\n",
        "        valid_blocks = [b for b in self.memory_blocks if b is not None]\n",
        "\n",
        "        # 수명 기준으로 정렬\n",
        "        valid_blocks.sort(key=lambda x: x[3])\n",
        "\n",
        "        # 메모리 재할당\n",
        "        self.memory_blocks = []\n",
        "        self.tensor_map.clear()\n",
        "        self.allocated_memory = 0\n",
        "\n",
        "        current_addr = 0\n",
        "        for _, size, tensor_id, lifetime in valid_blocks:\n",
        "            self.memory_blocks.append((current_addr, size, tensor_id, lifetime))\n",
        "            self.tensor_map[tensor_id] = len(self.memory_blocks) - 1\n",
        "            self.allocated_memory += size\n",
        "            current_addr += size\n",
        "\n",
        "    def _lifetimes_overlap(self, lifetime1, lifetime2):\n",
        "        \"\"\"두 텐서의 수명이 겹치는지 확인\"\"\"\n",
        "        start1, end1 = lifetime1\n",
        "        start2, end2 = lifetime2\n",
        "        return not (end1 <= start2 or end2 <= start1)"
      ],
      "metadata": {
        "id": "jYTJczzzX87b"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_keys = [\"forward_pass\", \"loss_computation\", \"backward_pass\", \"optimizer_step\"]"
      ],
      "metadata": {
        "id": "TchWyI-to2c6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MelonTrainer:\n",
        "    def __init__(self, model, criterion, optimizer, device, memory_budget):\n",
        "        self.model = model.to(device)\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.memory_budget = memory_budget\n",
        "        self.has_bn = self._check_has_bn()\n",
        "        self.memory_pool = self._initialize_memory_pool()\n",
        "\n",
        "    def _check_has_bn(self):\n",
        "        \"\"\"BatchNorm 레이어 존재 여부 확인\"\"\"\n",
        "        for module in self.model.modules():\n",
        "            if isinstance(module, nn.BatchNorm2d):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _initialize_memory_pool(self):\n",
        "        \"\"\"Lifetime-aware memory pool 초기화\"\"\"\n",
        "        return LifetimeAwareMemoryPool(self.memory_budget)\n",
        "\n",
        "    def train(self, train_loader):\n",
        "        start_time = time.monotonic()\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with profile(\n",
        "            activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "            profile_memory=True,  # 메모리 사용량 추적\n",
        "            record_shapes=True  # 텐서 크기 기록\n",
        "        ) as prof:\n",
        "\n",
        "            for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "                if self.has_bn:\n",
        "                  # BatchNorm이 있는 경우 recomputation 사용\n",
        "                  loss, acc = self._train_step_with_recomputation(inputs, labels)\n",
        "                else:\n",
        "                  # BatchNorm이 없는 경우 micro-batch 사용\n",
        "                  loss, acc = self._train_step_with_microbatch(inputs, labels)\n",
        "\n",
        "                running_loss += loss\n",
        "                correct += acc[0]\n",
        "                total += acc[1]\n",
        "\n",
        "        end_time = time.monotonic()\n",
        "\n",
        "        filtered_averages = [avg for avg in prof.key_averages() if avg.key in selected_keys]\n",
        "        extracted_data = []\n",
        "\n",
        "        for avg in filtered_averages:\n",
        "          avg_str = str(avg)\n",
        "          match = pattern.search(avg_str)\n",
        "          if match:\n",
        "            extracted_data.append(match.groupdict())\n",
        "        df = pd.DataFrame(extracted_data)\n",
        "        print(df)\n",
        "        free_memory, total_memory = torch.cuda.mem_get_info()\n",
        "        print(f\"Free memory: {free_memory / 1024**2:.2f} MB\")\n",
        "        print(f\"Total memory: {total_memory / 1024**2:.2f} MB\")\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        return epoch_loss, accuracy, start_time, end_time\n",
        "\n",
        "    def _train_step_with_recomputation(self, inputs, labels):\n",
        "        \"\"\"Recomputation을 사용한 학습 스텝\"\"\"\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass with checkpoints\n",
        "        with torch.no_grad():\n",
        "            intermediate_outputs = []\n",
        "            x = inputs\n",
        "\n",
        "            # ResNet의 각 레이어별로 순차적으로 처리\n",
        "            x = self.model.conv1(x)\n",
        "            x = self.model.bn1(x)\n",
        "            x = self.model.relu(x)\n",
        "            x = self.model.maxpool(x)\n",
        "\n",
        "            # layer1-4 처리\n",
        "            for layer_name in ['layer1', 'layer2', 'layer3', 'layer4']:\n",
        "                layer = getattr(self.model, layer_name)\n",
        "                x = layer(x)\n",
        "                if self.has_bn:\n",
        "                    intermediate_outputs.append(x.detach())\n",
        "\n",
        "        # Recomputation and backward\n",
        "        with record_function(\"forward_pass\"):\n",
        "            outputs = self.model(inputs)\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]\n",
        "        with record_function(\"loss_computation\"):\n",
        "            loss = self.criterion(outputs, labels)\n",
        "\n",
        "        with record_function(\"backward_pass\"):\n",
        "            loss.backward()\n",
        "        with record_function(\"optimizer_step\"):\n",
        "            self.optimizer.step()\n",
        "\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct = (predicted == labels).sum().item()\n",
        "        total = labels.size(0)\n",
        "\n",
        "        return loss.item(), (correct, total)\n",
        "\n",
        "    def _train_step_with_microbatch(self, inputs, labels):\n",
        "        \"\"\"Micro-batch를 사용한 학습 스텝\"\"\"\n",
        "        batch_size = inputs.size(0)\n",
        "        micro_batch_size = self._calculate_micro_batch_size(inputs.size())\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        for i in range(0, batch_size, micro_batch_size):\n",
        "            micro_inputs = inputs[i:i+micro_batch_size]\n",
        "            micro_labels = labels[i:i+micro_batch_size]\n",
        "\n",
        "            with record_function(\"forward_pass\"):\n",
        "                outputs = self.model(micro_inputs)\n",
        "\n",
        "            with record_function(\"loss_computation\"):\n",
        "                loss = self.criterion(outputs[0], micro_labels)\n",
        "                scaled_loss = loss * (micro_batch_size / batch_size)\n",
        "\n",
        "            with record_function(\"backward_pass\"):\n",
        "                scaled_loss.backward()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs[0].data, 1)\n",
        "            correct += (predicted == micro_labels).sum().item()\n",
        "\n",
        "        with record_function(\"optimizer_step\"):\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return total_loss, (correct, batch_size)\n",
        "\n",
        "    def _calculate_micro_batch_size(self, input_size):\n",
        "        \"\"\"메모리 예산에 따른 micro-batch 크기 계산\"\"\"\n",
        "        tensor_size = input_size[1] * input_size[2] * input_size[3] * 4\n",
        "        return max(1, min(input_size[0], self.memory_budget // tensor_size))"
      ],
      "metadata": {
        "id": "Sy0m7X3dVluD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "XcKNWVMyWIR8"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs[0], labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs[0], 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    # print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    return epoch_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "CBXgAgVO0G4p"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut_BteniXzv1",
        "outputId": "160620df-042b-4b78-8bda-a68ca0150516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free memory: 40026.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        }
      ],
      "source": [
        "free_memory, total_memory = torch.cuda.mem_get_info()\n",
        "print(f\"Free memory: {free_memory / 1024**2:.2f} MB\")\n",
        "print(f\"Total memory: {total_memory / 1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsG0saouWIR9",
        "outputId": "8f3d5efc-2e41-43ac-8b6c-bb996cc6a92a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [00:31<00:00,  9.99it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     657.853ms    8.380ms        0.000us    4.914ms   \n",
            "1      forward_pass       0.000us    0.000us         2.559s    8.177ms   \n",
            "2  loss_computation      25.877ms  477.618us        0.000us    5.570us   \n",
            "3  loss_computation       0.000us    0.000us       47.677ms  152.322us   \n",
            "4     backward_pass        3.581s   11.541ms        0.000us    1.766us   \n",
            "5     backward_pass       0.000us    0.000us      552.896us    1.766us   \n",
            "6    optimizer_step      15.736ms    3.143ms        0.000us  678.317us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216566887424>  \n",
            "1                             0                0>  \n",
            "2                             0           800768>  \n",
            "3                             0                0>  \n",
            "4                             0    -201847934976>  \n",
            "5                             0                0>  \n",
            "6                           248         91884544>  \n",
            "Free memory: 38510.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:04<00:00, 20.35it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Train Time: 1m 7s\n",
            "\tTrain Loss: 1.982 | Train Acc: 27.14%\n",
            "\t Val. Loss: 1.894 |  Val. Acc: 28.87%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [00:29<00:00, 10.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     647.334ms    7.876ms        0.000us    4.912ms   \n",
            "1      forward_pass       0.000us    0.000us         2.406s    7.688ms   \n",
            "2  loss_computation      24.663ms  189.568us        0.000us    5.570us   \n",
            "3  loss_computation       0.000us    0.000us       18.384ms   58.734us   \n",
            "4     backward_pass        3.051s    9.798ms        0.000us    1.768us   \n",
            "5     backward_pass       0.000us    0.000us      553.381us    1.768us   \n",
            "6    optimizer_step      15.407ms    2.597ms        0.000us  677.482us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216552928256>  \n",
            "1                             0                0>  \n",
            "2                             0           800768>  \n",
            "3                             0                0>  \n",
            "4                             0    -201856978944>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 38510.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:05<00:00, 17.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Epoch Train Time: 1m 4s\n",
            "\tTrain Loss: 1.782 | Train Acc: 34.05%\n",
            "\t Val. Loss: 1.629 |  Val. Acc: 39.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:27<00:00, 11.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     633.446ms    7.757ms        0.000us    4.913ms   \n",
            "1      forward_pass       0.000us    0.000us         2.374s    7.584ms   \n",
            "2  loss_computation      22.319ms  177.616us        0.000us    5.566us   \n",
            "3  loss_computation       0.000us    0.000us       17.771ms   56.777us   \n",
            "4     backward_pass        2.922s    9.382ms        0.000us    1.768us   \n",
            "5     backward_pass       0.000us    0.000us      553.474us    1.768us   \n",
            "6    optimizer_step      14.736ms    2.536ms        0.000us  675.993us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216552928256>  \n",
            "1                             0                0>  \n",
            "2                             0           800768>  \n",
            "3                             0                0>  \n",
            "4                             0    -201856978944>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 38510.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:05<00:00, 17.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Epoch Train Time: 1m 2s\n",
            "\tTrain Loss: 1.692 | Train Acc: 37.90%\n",
            "\t Val. Loss: 1.557 |  Val. Acc: 42.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:28<00:00, 11.09it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     639.099ms    7.806ms        0.000us    4.912ms   \n",
            "1      forward_pass       0.000us    0.000us         2.389s    7.632ms   \n",
            "2  loss_computation      22.935ms  180.699us        0.000us    5.566us   \n",
            "3  loss_computation       0.000us    0.000us       17.929ms   57.280us   \n",
            "4     backward_pass        2.926s    9.394ms        0.000us    1.767us   \n",
            "5     backward_pass       0.000us    0.000us      553.147us    1.767us   \n",
            "6    optimizer_step      14.794ms    2.563ms        0.000us  675.671us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216552928256>  \n",
            "1                             0                0>  \n",
            "2                             0           800768>  \n",
            "3                             0                0>  \n",
            "4                             0    -201856978944>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 38510.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:05<00:00, 16.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Epoch Train Time: 1m 2s\n",
            "\tTrain Loss: 1.601 | Train Acc: 41.67%\n",
            "\t Val. Loss: 1.441 |  Val. Acc: 47.83%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:28<00:00, 10.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     648.517ms    7.894ms        0.000us    4.912ms   \n",
            "1      forward_pass       0.000us    0.000us         2.415s    7.717ms   \n",
            "2  loss_computation      22.827ms  181.822us        0.000us    5.573us   \n",
            "3  loss_computation       0.000us    0.000us       18.108ms   57.854us   \n",
            "4     backward_pass        2.998s    9.627ms        0.000us    1.767us   \n",
            "5     backward_pass       0.000us    0.000us      552.986us    1.767us   \n",
            "6    optimizer_step      15.061ms    2.595ms        0.000us  675.518us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216552928256>  \n",
            "1                             0                0>  \n",
            "2                             0           800768>  \n",
            "3                             0                0>  \n",
            "4                             0    -201856978944>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 38510.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 15.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Epoch Train Time: 1m 3s\n",
            "\tTrain Loss: 1.496 | Train Acc: 46.12%\n",
            "\t Val. Loss: 1.342 |  Val. Acc: 51.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:29<00:00, 10.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     649.083ms    7.920ms        0.000us    4.913ms   \n",
            "1      forward_pass       0.000us    0.000us         2.424s    7.746ms   \n",
            "2  loss_computation      22.575ms  179.835us        0.000us    5.564us   \n",
            "3  loss_computation       0.000us    0.000us       17.952ms   57.354us   \n",
            "4     backward_pass        2.968s    9.531ms        0.000us    1.766us   \n",
            "5     backward_pass       0.000us    0.000us      552.870us    1.766us   \n",
            "6    optimizer_step      15.674ms    2.631ms        0.000us  675.981us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216552928256>  \n",
            "1                             0                0>  \n",
            "2                             0           800768>  \n",
            "3                             0                0>  \n",
            "4                             0    -201856978944>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 38510.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 15.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Epoch Train Time: 1m 4s\n",
            "\tTrain Loss: 1.370 | Train Acc: 51.24%\n",
            "\t Val. Loss: 1.222 |  Val. Acc: 54.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:31<00:00, 10.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     654.866ms    7.968ms        0.000us    4.913ms   \n",
            "1      forward_pass       0.000us    0.000us         2.437s    7.785ms   \n",
            "2  loss_computation      23.650ms  185.685us        0.000us    5.572us   \n",
            "3  loss_computation       0.000us    0.000us       18.227ms   58.233us   \n",
            "4     backward_pass        3.080s    9.891ms        0.000us    1.768us   \n",
            "5     backward_pass       0.000us    0.000us      553.474us    1.768us   \n",
            "6    optimizer_step      15.530ms    2.683ms        0.000us  675.784us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216552928256>  \n",
            "1                             0                0>  \n",
            "2                             0           800768>  \n",
            "3                             0                0>  \n",
            "4                             0    -201856978944>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 38510.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 14.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Epoch Train Time: 1m 5s\n",
            "\tTrain Loss: 1.251 | Train Acc: 55.55%\n",
            "\t Val. Loss: 1.058 |  Val. Acc: 62.17%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:31<00:00,  9.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     656.281ms    8.017ms        0.000us    4.913ms   \n",
            "1      forward_pass       0.000us    0.000us         2.454s    7.840ms   \n",
            "2  loss_computation      23.496ms  185.212us        0.000us    5.568us   \n",
            "3  loss_computation       0.000us    0.000us       18.375ms   58.708us   \n",
            "4     backward_pass        2.983s    9.579ms        0.000us    1.768us   \n",
            "5     backward_pass       0.000us    0.000us      553.496us    1.768us   \n",
            "6    optimizer_step      15.698ms    2.700ms        0.000us  676.098us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216552928256>  \n",
            "1                             0                0>  \n",
            "2                             0           800768>  \n",
            "3                             0                0>  \n",
            "4                             0    -201856978944>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 38510.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 15.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Epoch Train Time: 1m 6s\n",
            "\tTrain Loss: 1.166 | Train Acc: 58.03%\n",
            "\t Val. Loss: 0.994 |  Val. Acc: 64.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:31<00:00,  9.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     667.191ms    8.067ms        0.000us    4.913ms   \n",
            "1      forward_pass       0.000us    0.000us         2.466s    7.877ms   \n",
            "2  loss_computation      24.641ms  190.941us        0.000us    5.570us   \n",
            "3  loss_computation       0.000us    0.000us       18.700ms   59.744us   \n",
            "4     backward_pass        3.061s    9.831ms        0.000us    1.769us   \n",
            "5     backward_pass       0.000us    0.000us      553.595us    1.769us   \n",
            "6    optimizer_step      16.230ms    2.744ms        0.000us  675.789us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216552928256>  \n",
            "1                             0                0>  \n",
            "2                             0           800768>  \n",
            "3                             0                0>  \n",
            "4                             0    -201856978944>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 38510.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 14.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Epoch Train Time: 1m 6s\n",
            "\tTrain Loss: 1.079 | Train Acc: 61.09%\n",
            "\t Val. Loss: 1.034 |  Val. Acc: 63.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:32<00:00,  9.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     676.010ms    8.272ms        0.000us    4.912ms   \n",
            "1      forward_pass       0.000us    0.000us         2.529s    8.079ms   \n",
            "2  loss_computation      24.981ms  193.219us        0.000us    5.563us   \n",
            "3  loss_computation       0.000us    0.000us       18.738ms   59.864us   \n",
            "4     backward_pass        3.113s    9.998ms        0.000us    1.767us   \n",
            "5     backward_pass       0.000us    0.000us      553.092us    1.767us   \n",
            "6    optimizer_step      16.603ms    2.823ms        0.000us  675.856us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216552928256>  \n",
            "1                             0                0>  \n",
            "2                             0           800768>  \n",
            "3                             0                0>  \n",
            "4                             0    -201856978944>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 38510.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:05<00:00, 15.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Epoch Train Time: 1m 8s\n",
            "\tTrain Loss: 1.000 | Train Acc: 64.56%\n",
            "\t Val. Loss: 0.875 |  Val. Acc: 68.27%\n",
            "Train finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "trainer = MelonTrainer(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device=device,\n",
        "    memory_budget=4096\n",
        ")\n",
        "\n",
        "EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "total_time = 0\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc, start_time, end_time = trainer.train(train_iterator)\n",
        "\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    total_time += end_time - start_time\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Train Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc:.2f}%')\n",
        "\n",
        "print(\"Train finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mddh-LmVYhVe",
        "outputId": "b3d5969f-82d2-4dd6-cf05-eed43d8388cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free memory: 38510.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        }
      ],
      "source": [
        "free_memory, total_memory = torch.cuda.mem_get_info()\n",
        "print(f\"Free memory: {free_memory / 1024**2:.2f} MB\")\n",
        "print(f\"Total memory: {total_memory / 1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YfjvTjYWIR9",
        "outputId": "18492835-11b9-4462-c814-590547938aa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet18\n",
            "Total Training Time: 10m 52s\n"
          ]
        }
      ],
      "source": [
        "print(\"ResNet18\")\n",
        "print(f'Total Training Time: {int(total_time/60)}m {int(total_time%60)}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "GG41xbRBaqEt"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'trained_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPqa67aoatE7",
        "outputId": "3a7ee4ba-50ba-40e9-8c6b-6d086c05940f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model file size: 42.73 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "model_file_size = os.path.getsize('trained_model.pth')  # 바이트 단위\n",
        "model_file_size_MB = model_file_size / (1024 ** 2)  # MB로 변환\n",
        "print(f\"Saved model file size: {model_file_size_MB:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcaaiNYha3pK",
        "outputId": "2a0b43d3-0451-456a-93e4-06e4da1518c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight - Size: torch.Size([64, 3, 7, 7]) - Number of elements: 9408\n",
            "bn1.weight - Size: torch.Size([64]) - Number of elements: 64\n",
            "bn1.bias - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.0.conv1.weight - Size: torch.Size([64, 64, 3, 3]) - Number of elements: 36864\n",
            "layer1.0.bn1.weight - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.0.bn1.bias - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.0.conv2.weight - Size: torch.Size([64, 64, 3, 3]) - Number of elements: 36864\n",
            "layer1.0.bn2.weight - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.0.bn2.bias - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.1.conv1.weight - Size: torch.Size([64, 64, 3, 3]) - Number of elements: 36864\n",
            "layer1.1.bn1.weight - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.1.bn1.bias - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.1.conv2.weight - Size: torch.Size([64, 64, 3, 3]) - Number of elements: 36864\n",
            "layer1.1.bn2.weight - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.1.bn2.bias - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer2.0.conv1.weight - Size: torch.Size([128, 64, 3, 3]) - Number of elements: 73728\n",
            "layer2.0.bn1.weight - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.0.bn1.bias - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.0.conv2.weight - Size: torch.Size([128, 128, 3, 3]) - Number of elements: 147456\n",
            "layer2.0.bn2.weight - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.0.bn2.bias - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.0.downsample.0.weight - Size: torch.Size([128, 64, 1, 1]) - Number of elements: 8192\n",
            "layer2.0.downsample.1.weight - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.0.downsample.1.bias - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.1.conv1.weight - Size: torch.Size([128, 128, 3, 3]) - Number of elements: 147456\n",
            "layer2.1.bn1.weight - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.1.bn1.bias - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.1.conv2.weight - Size: torch.Size([128, 128, 3, 3]) - Number of elements: 147456\n",
            "layer2.1.bn2.weight - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.1.bn2.bias - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer3.0.conv1.weight - Size: torch.Size([256, 128, 3, 3]) - Number of elements: 294912\n",
            "layer3.0.bn1.weight - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.0.bn1.bias - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.0.conv2.weight - Size: torch.Size([256, 256, 3, 3]) - Number of elements: 589824\n",
            "layer3.0.bn2.weight - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.0.bn2.bias - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.0.downsample.0.weight - Size: torch.Size([256, 128, 1, 1]) - Number of elements: 32768\n",
            "layer3.0.downsample.1.weight - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.0.downsample.1.bias - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.1.conv1.weight - Size: torch.Size([256, 256, 3, 3]) - Number of elements: 589824\n",
            "layer3.1.bn1.weight - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.1.bn1.bias - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.1.conv2.weight - Size: torch.Size([256, 256, 3, 3]) - Number of elements: 589824\n",
            "layer3.1.bn2.weight - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.1.bn2.bias - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer4.0.conv1.weight - Size: torch.Size([512, 256, 3, 3]) - Number of elements: 1179648\n",
            "layer4.0.bn1.weight - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.0.bn1.bias - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.0.conv2.weight - Size: torch.Size([512, 512, 3, 3]) - Number of elements: 2359296\n",
            "layer4.0.bn2.weight - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.0.bn2.bias - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.0.downsample.0.weight - Size: torch.Size([512, 256, 1, 1]) - Number of elements: 131072\n",
            "layer4.0.downsample.1.weight - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.0.downsample.1.bias - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.1.conv1.weight - Size: torch.Size([512, 512, 3, 3]) - Number of elements: 2359296\n",
            "layer4.1.bn1.weight - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.1.bn1.bias - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.1.conv2.weight - Size: torch.Size([512, 512, 3, 3]) - Number of elements: 2359296\n",
            "layer4.1.bn2.weight - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.1.bn2.bias - Size: torch.Size([512]) - Number of elements: 512\n",
            "fc.weight - Size: torch.Size([10, 512]) - Number of elements: 5120\n",
            "fc.bias - Size: torch.Size([10]) - Number of elements: 10\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"{name} - Size: {param.size()} - Number of elements: {param.numel()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Biwb-ebya6TH",
        "outputId": "ea0024f1-ded3-473e-e213-ffd0f3e9dfeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 11181642\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JEUocY4bDmO",
        "outputId": "e9d0d5d5-530a-4807-972f-766f820b0965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,181,642\n",
            "Trainable params: 11,181,642\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.65\n",
            "Estimated Total Size (MB): 106.01\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model, input_size=(3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import profiler\n",
        "dummy_input = torch.randn(32, 3, 224, 224).cuda()\n",
        "\n",
        "# Profiling inference\n",
        "with profiler.profile(\n",
        "    activities=[\n",
        "       profiler.ProfilerActivity.CPU,\n",
        "        profiler.ProfilerActivity.CUDA,  # Include if using GPU\n",
        "    ],\n",
        "    on_trace_ready=profiler.tensorboard_trace_handler(\"./logs\"),  # Optional logging\n",
        "    record_shapes=True,\n",
        "    with_stack=True\n",
        ") as prof:\n",
        "    with torch.no_grad():\n",
        "        model(dummy_input)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\", row_limit=50))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF4YJNY2Db_p",
        "outputId": "e9277956-0427-4284-9e8f-b6ed85fd71b6"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::conv2d         2.04%     145.925us        58.13%       4.152ms     207.589us       0.000us         0.00%       2.719ms     135.926us            20  \n",
            "                                      aten::convolution         3.19%     228.119us        56.09%       4.006ms     200.293us       0.000us         0.00%       2.719ms     135.926us            20  \n",
            "                                     aten::_convolution         3.18%     227.358us        52.90%       3.778ms     188.887us       0.000us         0.00%       2.719ms     135.926us            20  \n",
            "                                aten::cudnn_convolution        30.60%       2.186ms        49.71%       3.550ms     177.519us       2.719ms        65.45%       2.719ms     135.926us            20  \n",
            "void cudnn::engines_precompiled::nchwToNhwcKernel<fl...         0.00%       0.000us         0.00%       0.000us       0.000us     659.553us        15.88%     659.553us      17.357us            38  \n",
            "sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     658.629us        15.86%     658.629us      65.863us            10  \n",
            "                                       aten::batch_norm         1.13%      80.951us        27.21%       1.943ms      97.154us       0.000us         0.00%     645.282us      32.264us            20  \n",
            "                           aten::_batch_norm_impl_index         1.41%     100.355us        26.07%       1.862ms      93.106us       0.000us         0.00%     645.282us      32.264us            20  \n",
            "                                 aten::cudnn_batch_norm        10.73%     766.222us        24.67%       1.762ms      88.088us     645.282us        15.54%     645.282us      32.264us            20  \n",
            "void cudnn::bn_fw_inf_1C11_kernel_NCHW<float, float,...         0.00%       0.000us         0.00%       0.000us       0.000us     645.282us        15.54%     645.282us      32.264us            20  \n",
            "sm80_xmma_fprop_implicit_gemm_indexed_tf32f32_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     498.531us        12.00%     498.531us     124.633us             4  \n",
            "                                            aten::relu_         1.71%     122.343us         7.12%     508.555us      29.915us       0.000us         0.00%     369.380us      21.728us            17  \n",
            "                                       aten::clamp_min_         3.17%     226.264us         5.41%     386.212us      22.718us     369.380us         8.89%     369.380us      21.728us            17  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     369.380us         8.89%     369.380us      21.728us            17  \n",
            "sm80_xmma_fprop_implicit_gemm_indexed_wo_smem_tf32f3...         0.00%       0.000us         0.00%       0.000us       0.000us     352.193us         8.48%     352.193us     352.193us             1  \n",
            "void cutlass__5x_cudnn::Kernel<cutlass_tensorop_s168...         0.00%       0.000us         0.00%       0.000us       0.000us     263.298us         6.34%     263.298us      87.766us             3  \n",
            "                                       aten::max_pool2d         0.19%      13.322us         0.90%      63.978us      63.978us       0.000us         0.00%     207.873us     207.873us             1  \n",
            "                          aten::max_pool2d_with_indices         0.52%      37.118us         0.71%      50.656us      50.656us     207.873us         5.00%     207.873us     207.873us             1  \n",
            "void at::native::(anonymous namespace)::max_pool_for...         0.00%       0.000us         0.00%       0.000us       0.000us     207.873us         5.00%     207.873us     207.873us             1  \n",
            "                                             aten::add_         1.97%     140.486us         3.01%     214.819us      26.852us     188.288us         4.53%     188.288us      23.536us             8  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us     188.288us         4.53%     188.288us      23.536us             8  \n",
            "void cudnn::engines_precompiled::nhwcToNchwKernel<fl...         0.00%       0.000us         0.00%       0.000us       0.000us     177.922us         4.28%     177.922us      35.584us             5  \n",
            "sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us      54.753us         1.32%      54.753us      54.753us             1  \n",
            "       _5x_cudnn_ampere_scudnn_128x64_relu_medium_nn_v1         0.00%       0.000us         0.00%       0.000us       0.000us      50.496us         1.22%      50.496us      50.496us             1  \n",
            "                              aten::adaptive_avg_pool2d         0.09%       6.229us         0.74%      53.034us      53.034us       0.000us         0.00%      12.384us      12.384us             1  \n",
            "                                             aten::mean         0.49%      34.748us         0.66%      46.805us      46.805us      12.384us         0.30%      12.384us      12.384us             1  \n",
            "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      12.384us         0.30%      12.384us      12.384us             1  \n",
            "                                           aten::linear         0.14%       9.740us         2.36%     168.350us     168.350us       0.000us         0.00%      11.712us      11.712us             1  \n",
            "                                            aten::addmm         1.58%     112.495us         1.97%     140.751us     140.751us      11.712us         0.28%      11.712us      11.712us             1  \n",
            "                        ampere_sgemm_32x32_sliced1x4_tn         0.00%       0.000us         0.00%       0.000us       0.000us       7.872us         0.19%       7.872us       7.872us             1  \n",
            "void splitKreduce_kernel<32, 16, int, float, float, ...         0.00%       0.000us         0.00%       0.000us       0.000us       3.840us         0.09%       3.840us       3.840us             1  \n",
            "void cask__5x_cudnn::computeOffsetsKernel<false, fal...         0.00%       0.000us         0.00%       0.000us       0.000us       3.136us         0.08%       3.136us       3.136us             1  \n",
            "                                        cudaEventRecord         1.48%     105.389us         1.48%     105.389us       2.635us       0.000us         0.00%       0.000us       0.000us            40  \n",
            "                                  cudaStreamIsCapturing         0.45%      32.319us         0.45%      32.319us       0.808us       0.000us         0.00%       0.000us       0.000us            40  \n",
            "                                  cudaStreamGetPriority         0.40%      28.800us         0.40%      28.800us       0.720us       0.000us         0.00%       0.000us       0.000us            40  \n",
            "                       cudaDeviceGetStreamPriorityRange         0.34%      24.424us         0.34%      24.424us       0.611us       0.000us         0.00%       0.000us       0.000us            40  \n",
            "                                       cudaLaunchKernel        22.95%       1.639ms        22.95%       1.639ms      16.899us       0.000us         0.00%       0.000us       0.000us            97  \n",
            "                                    cudaPeekAtLastError         0.12%       8.452us         0.12%       8.452us       0.211us       0.000us         0.00%       0.000us       0.000us            40  \n",
            "                                    cudaLaunchKernelExC         1.66%     118.502us         1.66%     118.502us       7.406us       0.000us         0.00%       0.000us       0.000us            16  \n",
            "                                       aten::empty_like         1.48%     105.426us         4.21%     300.309us      15.015us       0.000us         0.00%       0.000us       0.000us            20  \n",
            "                                            aten::empty         6.86%     489.948us         6.86%     489.948us       6.124us       0.000us         0.00%       0.000us       0.000us            80  \n",
            "                                             aten::view         1.09%      77.860us         1.09%      77.860us       3.708us       0.000us         0.00%       0.000us       0.000us            21  \n",
            "                                   cudaFuncSetAttribute         0.54%      38.618us         0.54%      38.618us       1.170us       0.000us         0.00%       0.000us       0.000us            33  \n",
            "                                        cudaMemsetAsync         0.01%       0.535us         0.01%       0.535us       0.535us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                                aten::t         0.11%       7.708us         0.25%      17.859us      17.859us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                        aten::transpose         0.10%       7.175us         0.14%      10.151us      10.151us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                       aten::as_strided         0.04%       2.976us         0.04%       2.976us       2.976us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "                                  cudaDeviceSynchronize         0.24%      16.957us         0.24%      16.957us      16.957us       0.000us         0.00%       0.000us       0.000us             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 7.142ms\n",
            "Self CUDA time total: 4.153ms\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}