{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZjVr5KDBKF2H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.profiler import profile, record_function, ProfilerActivity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xPgeN2tuKF2I"
      },
      "outputs": [],
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleModel, self).__init__()\n",
        "        self.layer1 = nn.Linear(512, 256)\n",
        "        self.layer2 = nn.Linear(256, 128)\n",
        "        self.layer3 = nn.Linear(128, 64)\n",
        "        self.layer4 = nn.Linear(64, 10)  # Output layer for 10 classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        x = torch.relu(self.layer3(x))\n",
        "        x = self.layer4(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Q7ZsrGGbKF2I"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleModel().to(device)\n",
        "input_tensor = torch.randn(64, 512).to(device)  # Batch size 64, input size 512\n",
        "labels = torch.randint(0, 10, (64,)).to(device)  # 랜덤 라벨\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rdhOx7jDKF2I"
      },
      "outputs": [],
      "source": [
        "with profile(\n",
        "    activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "    profile_memory=True,\n",
        "    record_shapes=True\n",
        ") as prof:\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    input_tensor = torch.randn(64, 512).to(device)\n",
        "    labels = torch.randint(0, 10, (64,)).to(device)\n",
        "\n",
        "    for name, layer in model.named_children():\n",
        "        with record_function(f\"Forward_{name}\"):\n",
        "            input_tensor = layer(input_tensor)\n",
        "\n",
        "    loss = criterion(input_tensor, labels)\n",
        "\n",
        "    with record_function(\"Backward\"):\n",
        "        loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5ysce9WKF2J",
        "outputId": "5dcb61ad-0414-41bf-ae20-6a10d01fc45d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                         AddmmBackward0         0.99%       4.423ms         3.10%      13.806ms       3.452ms       0.000us         0.00%      55.616us      13.904us           0 b           0 b       8.89 Mb           0 b             4  \n",
            "                                               aten::mm         1.92%       8.545ms         2.08%       9.253ms       1.322ms      55.616us        29.82%      55.616us       7.945us           0 b           0 b       8.89 Mb       8.89 Mb             7  \n",
            "    autograd::engine::evaluate_function: AddmmBackward0         0.05%     222.792us        12.13%      54.052ms      13.513ms       0.000us         0.00%      86.849us      21.712us           0 b           0 b       8.55 Mb    -354.50 Kb             4  \n",
            "                                               Backward        14.98%      66.783ms        18.53%      82.617ms      82.617ms       0.000us         0.00%       1.632us       1.632us           0 b           0 b       8.55 Mb       8.55 Mb             1  \n",
            "                                           aten::linear         1.49%       6.658ms        31.52%     140.531ms      35.133ms       0.000us         0.00%      47.040us      11.760us           0 b           0 b       8.24 Mb           0 b             4  \n",
            "                                            aten::addmm        13.86%      61.796ms        28.96%     129.088ms      32.272ms      47.040us        25.22%      47.040us      11.760us           0 b           0 b       8.24 Mb       8.24 Mb             4  \n",
            "                                         Forward_layer1         0.05%     224.835us        31.48%     140.349ms     140.349ms       0.000us         0.00%      17.792us      17.792us           0 b           0 b       8.19 Mb           0 b             1  \n",
            "                                    aten::empty_strided         0.02%      90.821us         0.02%      90.821us      30.274us       0.000us         0.00%       0.000us       0.000us           0 b           0 b     129.00 Kb     129.00 Kb             3  \n",
            "                                               aten::to         1.13%       5.056ms         1.18%       5.281ms       1.760ms       0.000us         0.00%      15.840us       5.280us           0 b           0 b     128.50 Kb           0 b             3  \n",
            "                                         aten::_to_copy         0.01%      33.374us         0.05%     224.848us     112.424us       0.000us         0.00%      15.840us       7.920us           0 b           0 b     128.50 Kb           0 b             2  \n",
            "                                         Forward_layer2         0.03%     152.221us         0.08%     373.955us     373.955us       0.000us         0.00%      11.808us      11.808us           0 b           0 b      32.00 Kb           0 b             1  \n",
            "                                         Forward_layer3         0.02%      70.852us         0.04%     177.223us     177.223us       0.000us         0.00%      10.080us      10.080us           0 b           0 b      16.00 Kb           0 b             1  \n",
            "                               aten::cross_entropy_loss         0.81%       3.618ms        25.29%     112.732ms     112.732ms       0.000us         0.00%       7.392us       7.392us           0 b           0 b       3.50 Kb           0 b             1  \n",
            "                                         Forward_layer4         0.01%      53.120us         0.03%     132.223us     132.223us       0.000us         0.00%       7.360us       7.360us           0 b           0 b       2.50 Kb           0 b             1  \n",
            "                                      aten::log_softmax         0.04%     159.043us        16.29%      72.623ms      72.623ms       0.000us         0.00%       2.944us       2.944us           0 b           0 b       2.50 Kb           0 b             1  \n",
            "                                     aten::_log_softmax         0.62%       2.783ms        16.26%      72.462ms      72.462ms       2.944us         1.58%       2.944us       2.944us           0 b           0 b       2.50 Kb       2.50 Kb             1  \n",
            "                                       NllLossBackward0         0.48%       2.162ms         0.71%       3.156ms       3.156ms       0.000us         0.00%       5.728us       5.728us           0 b           0 b       2.50 Kb           0 b             1  \n",
            "                                aten::nll_loss_backward         0.18%     807.329us         0.22%     994.324us     994.324us       3.072us         1.65%       5.728us       5.728us           0 b           0 b       2.50 Kb       2.50 Kb             1  \n",
            "                                    LogSoftmaxBackward0         0.03%     119.825us         0.08%     335.666us     335.666us       0.000us         0.00%       3.104us       3.104us           0 b           0 b       2.50 Kb           0 b             1  \n",
            "                       aten::_log_softmax_backward_data         0.04%     156.259us         0.05%     215.841us     215.841us       3.104us         1.66%       3.104us       3.104us           0 b           0 b       2.50 Kb       2.50 Kb             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 445.782ms\n",
            "Self CUDA time total: 186.529us\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(prof.key_averages().table(sort_by=\"cuda_memory_usage\", row_limit=20))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WX7gDG9KF2J",
        "outputId": "340796dc-a857-4053-9d26-b82cb3df0e3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 메모리 사용량: 17.58 MB\n",
            "GPU 캐시 메모리 사용량: 22.00 MB\n"
          ]
        }
      ],
      "source": [
        "print(f\"GPU 메모리 사용량: {torch.cuda.memory_allocated() / (1024**2):.2f} MB\")\n",
        "print(f\"GPU 캐시 메모리 사용량: {torch.cuda.memory_reserved() / (1024**2):.2f} MB\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}