{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1UXBZ2kzoX1",
        "outputId": "6960d957-f6d6-4345-ff61-748b4bca26e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pynvml\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynvml\n",
            "Successfully installed pynvml-11.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iR74754xWIR3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pynvml import *\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KutboPCzza4_"
      },
      "outputs": [],
      "source": [
        "def print_gpu_utilization():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.cuda.current_device()  # 현재 GPU 디바이스 정보\n",
        "        allocated_memory = torch.cuda.memory_allocated(device) / 1024**3  # 메모리 사용량 (GB)\n",
        "        reserved_memory = torch.cuda.memory_reserved(device) / 1024**3  # 예약된 메모리 (GB)\n",
        "        print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
        "        print(f\"Reserved Memory: {reserved_memory:.2f} GB\")\n",
        "    else:\n",
        "        print(\"No GPU available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_8xbvf80zvjG"
      },
      "outputs": [],
      "source": [
        "def print_summary(result):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ntBtU40OWIR5"
      },
      "outputs": [],
      "source": [
        "size = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NkTBa1iCWIR5"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size, scale=(0.5, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68SaArxkWIR5",
        "outputId": "1561898a-0f66-481c-a5a2-afd3c521f3ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-10\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HISYmEIlWIR6"
      },
      "outputs": [],
      "source": [
        "VALID_RATIO = 0.7\n",
        "n_train_examples = int(len(trainset) * VALID_RATIO)\n",
        "n_valid_examples = len(trainset) - n_train_examples\n",
        "\n",
        "train_data, valid_data = data.random_split(trainset, [n_train_examples, n_valid_examples])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "nKkwwymDWIR6"
      },
      "outputs": [],
      "source": [
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transform = test_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kZE0KyvWIR6",
        "outputId": "2b8dddcc-7eff-410d-f88f-4768038a1cb8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35000, 15000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(train_data), len(valid_data), len(testset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NgFbnxO2WIR7"
      },
      "outputs": [],
      "source": [
        "sample_fraction = 0.2\n",
        "\n",
        "# 무작위 인덱스 생성\n",
        "train_indices = torch.randperm(len(trainset))[:int(len(trainset) * sample_fraction)]\n",
        "valid_indices = torch.randperm(len(valid_data))[:int(len(valid_data) * sample_fraction)]\n",
        "test_indices = torch.randperm(len(testset))[:int(len(testset) * sample_fraction)]\n",
        "\n",
        "# 서브셋 생성\n",
        "train_subset = Subset(trainset, train_indices)\n",
        "valid_subset = Subset(valid_data, valid_indices)\n",
        "test_subset = Subset(testset, test_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UMCcWRjWIR7",
        "outputId": "bfce53b2-8e00-490f-c476-78298d8579d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 3000, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(train_subset), len(valid_subset), len(test_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "KIuqCxeGWIR7"
      },
      "outputs": [],
      "source": [
        "train_iterator = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "valid_iterator = DataLoader(valid_subset, batch_size=batch_size, shuffle=False)\n",
        "test_iterator = DataLoader(test_subset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "6mVcfuK7WIR7"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample = False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if downsample:\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            downsample = None\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        i = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            i = self.downsample(i)\n",
        "\n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Ijsu-aH8WIR7"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, config, output_dim, zero_init_residual = False):\n",
        "        super().__init__()\n",
        "\n",
        "        block, n_blocks, channels = config\n",
        "        self.in_channels = channels[0]\n",
        "        assert len(n_blocks) == len(channels) == 4\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
        "        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride=2)\n",
        "        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride=2)\n",
        "        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(self.in_channels, output_dim)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "                #elif isinstance(m, Bottleneck):\n",
        "                    #nn.init.constant_(m.bn3.weight, 0)\n",
        "\n",
        "    def get_resnet_layer(self, block, n_blocks, channels, stride=1):\n",
        "        layers = []\n",
        "        if self.in_channels != block.expansion * channels:\n",
        "            downsample = True\n",
        "        else:\n",
        "            downsample = False\n",
        "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "        for i in range(1, n_blocks):\n",
        "            layers.append(block(block.expansion * channels, channels))\n",
        "\n",
        "        self.in_channels = block.expansion * channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        h = x.view(x.shape[0], -1)\n",
        "        x = self.fc(h)\n",
        "        return x, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fZ2g6Vv5WIR8"
      },
      "outputs": [],
      "source": [
        "ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "JyqkNDbOWIR8"
      },
      "outputs": [],
      "source": [
        "resnet18_config = ResNetConfig(block = BasicBlock, n_blocks = [2, 2, 2, 2], channels = [64, 128, 256, 512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "CGaz_A9yWIR8"
      },
      "outputs": [],
      "source": [
        "model = ResNet(resnet18_config, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf05XuEVY50-",
        "outputId": "168dc8c7-8f4a-49c7-aee5-9c271afa8632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "I-fZ50Ozf97u"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0fCSieg3WIR8"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "pretrained_model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "GwVmEYKxWIR8"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim=True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "DrpTHIkxQYuq"
      },
      "outputs": [],
      "source": [
        "pattern = re.compile(r'key=(?P<key>\\S+)\\s+'\n",
        "                     r'self_cpu_time=(?P<self_cpu_time>\\S+)\\s+'\n",
        "                     r'cpu_time=(?P<cpu_time>\\S+)\\s+'\n",
        "                     r'self_cuda_time=(?P<self_cuda_time>\\S+)\\s+'\n",
        "                     r'cuda_time=(?P<cuda_time>\\S+)\\s+'\n",
        "                     r'input_shapes=(?P<input_shapes>\\S*)\\s*'\n",
        "                     r'cpu_memory_usage=(?P<cpu_memory_usage>\\S*)\\s*'\n",
        "                     r'cuda_memory_usage=(?P<cuda_memory_usage>\\S*)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ElRmHAmtWIR8"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device, k):\n",
        "    start_time = time.monotonic()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with profile(\n",
        "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "        profile_memory=True,  # 메모리 사용량 추적\n",
        "        record_shapes=True  # 텐서 크기 기록\n",
        "    ) as prof:\n",
        "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # 기울기 초기화\n",
        "\n",
        "            with record_function(\"forward_pass\"):  # Forward pass 프로파일링\n",
        "                outputs = model(inputs)  # 모델 연산\n",
        "\n",
        "            with record_function(\"loss_computation\"):  # 손실 계산 프로파일링\n",
        "                loss = criterion(outputs[0], labels)  # 손실 계산\n",
        "\n",
        "            with record_function(\"backward_pass\"):  # Backward pass 프로파일링\n",
        "                loss.backward()  # 역전파\n",
        "\n",
        "            for i, param in enumerate(model.parameters()):\n",
        "                if i == len(list(model.parameters())) - 1:\n",
        "                    break\n",
        "                if param.grad is not None:\n",
        "                # 기울기의 절댓값을 기준으로 상위 k개 기울기 추적\n",
        "                    grad_values = param.grad.abs().view(-1)\n",
        "                    #print(len(grad_values))\n",
        "                    topk_values, _ = grad_values.topk(k, largest=True)\n",
        "\n",
        "                    threshold = topk_values[-1]\n",
        "\n",
        "                    # 임계값 이상이면 해당 기울기로 업데이트, 아니면 랜덤 값으로 대체\n",
        "                    mask = param.grad.abs() >= threshold\n",
        "\n",
        "                    updated_grad = torch.zeros_like(param.grad)\n",
        "                    updated_grad[mask] = param.grad[mask]\n",
        "\n",
        "                    del param.grad\n",
        "                    torch.cuda.empty_cache()\n",
        "                    param.grad = updated_grad.clone().detach()\n",
        "\n",
        "            with record_function(\"optimizer_step\"):\n",
        "                optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs[0], 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    end_time = time.monotonic()\n",
        "    selected_keys = [\"forward_pass\", \"loss_computation\", \"backward_pass\", \"optimizer_step\"]\n",
        "\n",
        "    # key_averages()로부터 얻은 평균값을 필터링\n",
        "    filtered_averages = [avg for avg in prof.key_averages() if avg.key in selected_keys]\n",
        "    extracted_data = []\n",
        "\n",
        "    for avg in filtered_averages:\n",
        "      avg_str = str(avg)\n",
        "      match = pattern.search(avg_str)\n",
        "      if match:\n",
        "        extracted_data.append(match.groupdict())\n",
        "    df = pd.DataFrame(extracted_data)\n",
        "    print(df)\n",
        "    free_memory, total_memory = torch.cuda.mem_get_info()\n",
        "    print(f\"Free memory: {free_memory / 1024**2:.2f} MB\")\n",
        "    print(f\"Total memory: {total_memory / 1024**2:.2f} MB\")\n",
        "    # 훈련 후 평균 손실과 정확도 계산\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, accuracy, start_time, end_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "XcKNWVMyWIR8"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs[0], labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs[0], 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    # print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    return epoch_loss, accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "CBXgAgVO0G4p"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ut_BteniXzv1",
        "outputId": "e94e5115-b33f-410e-f316-73eb19407893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free memory: 40026.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        }
      ],
      "source": [
        "free_memory, total_memory = torch.cuda.mem_get_info()\n",
        "print(f\"Free memory: {free_memory / 1024**2:.2f} MB\")\n",
        "print(f\"Total memory: {total_memory / 1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsG0saouWIR9",
        "outputId": "a6968b31-3ec6-4f34-ef16-81486c97480c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:50<00:00,  6.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     784.014ms   15.026ms        0.000us    4.085ms   \n",
            "1      forward_pass       0.000us    0.000us         3.952s   12.587ms   \n",
            "2  loss_computation      30.234ms  465.949us        0.000us    4.710us   \n",
            "3  loss_computation       0.000us    0.000us       43.413ms  138.700us   \n",
            "4     backward_pass        4.541s   14.602ms        0.000us    1.364us   \n",
            "5     backward_pass       0.000us    0.000us      426.903us    1.364us   \n",
            "6    optimizer_step      17.969ms    3.328ms        0.000us  696.919us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216404665344>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -201743044096>  \n",
            "5                             0                0>  \n",
            "6                           248         91950080>  \n",
            "Free memory: 39700.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:04<00:00, 19.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Train Time: 1m 56s\n",
            "\tTrain Loss: 2.153 | Train Acc: 19.15%\n",
            "\t Val. Loss: 1.960 |  Val. Acc: 25.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:48<00:00,  6.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     784.386ms   12.190ms        0.000us    4.079ms   \n",
            "1      forward_pass       0.000us    0.000us         3.553s   11.352ms   \n",
            "2  loss_computation      27.962ms  201.950us        0.000us    4.941us   \n",
            "3  loss_computation       0.000us    0.000us       18.492ms   59.079us   \n",
            "4     backward_pass        4.039s   12.955ms        0.000us    1.379us   \n",
            "5     backward_pass       0.000us    0.000us      431.517us    1.379us   \n",
            "6    optimizer_step      18.231ms    2.982ms        0.000us  694.314us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216396866560>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -201752809472>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39680.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 15.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Epoch Train Time: 1m 53s\n",
            "\tTrain Loss: 1.907 | Train Acc: 27.38%\n",
            "\t Val. Loss: 1.871 |  Val. Acc: 29.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:48<00:00,  6.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     766.241ms   11.802ms        0.000us    4.080ms   \n",
            "1      forward_pass       0.000us    0.000us         3.451s   11.025ms   \n",
            "2  loss_computation      26.295ms  193.684us        0.000us    4.736us   \n",
            "3  loss_computation       0.000us    0.000us       18.185ms   58.099us   \n",
            "4     backward_pass        3.917s   12.566ms        0.000us    1.363us   \n",
            "5     backward_pass       0.000us    0.000us      426.684us    1.363us   \n",
            "6    optimizer_step      17.964ms    2.974ms        0.000us  701.461us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216396735488>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -201751629824>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39700.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 15.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Epoch Train Time: 1m 52s\n",
            "\tTrain Loss: 1.849 | Train Acc: 29.01%\n",
            "\t Val. Loss: 1.806 |  Val. Acc: 30.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:49<00:00,  6.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     766.155ms   11.803ms        0.000us    4.089ms   \n",
            "1      forward_pass       0.000us    0.000us         3.451s   11.027ms   \n",
            "2  loss_computation      26.343ms  192.750us        0.000us    4.902us   \n",
            "3  loss_computation       0.000us    0.000us       18.029ms   57.602us   \n",
            "4     backward_pass        3.964s   12.712ms        0.000us    1.363us   \n",
            "5     backward_pass       0.000us    0.000us      426.631us    1.363us   \n",
            "6    optimizer_step      18.164ms    2.969ms        0.000us  700.295us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216396866560>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -201752809472>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39680.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:07<00:00, 12.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Epoch Train Time: 1m 53s\n",
            "\tTrain Loss: 1.824 | Train Acc: 29.94%\n",
            "\t Val. Loss: 1.760 |  Val. Acc: 32.80%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:56<00:00,  5.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     785.300ms   12.118ms        0.000us    4.531ms   \n",
            "1      forward_pass       0.000us    0.000us         3.554s   11.355ms   \n",
            "2  loss_computation      26.500ms  195.251us        0.000us    5.508us   \n",
            "3  loss_computation       0.000us    0.000us       18.459ms   58.973us   \n",
            "4     backward_pass        3.988s   12.794ms        0.000us    1.577us   \n",
            "5     backward_pass       0.000us    0.000us      493.559us    1.577us   \n",
            "6    optimizer_step      17.460ms    3.076ms        0.000us  729.473us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216396735488>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -201751629824>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39700.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 13.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Epoch Train Time: 2m 0s\n",
            "\tTrain Loss: 1.790 | Train Acc: 31.67%\n",
            "\t Val. Loss: 1.714 |  Val. Acc: 34.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:50<00:00,  6.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     763.982ms   11.879ms        0.000us    4.092ms   \n",
            "1      forward_pass       0.000us    0.000us         3.481s   11.121ms   \n",
            "2  loss_computation      26.023ms  190.956us        0.000us    4.687us   \n",
            "3  loss_computation       0.000us    0.000us       17.697ms   56.541us   \n",
            "4     backward_pass        3.918s   12.567ms        0.000us    1.367us   \n",
            "5     backward_pass       0.000us    0.000us      427.813us    1.367us   \n",
            "6    optimizer_step      17.004ms    2.948ms        0.000us  708.914us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216396866560>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -201752809472>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39680.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:08<00:00, 11.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Epoch Train Time: 1m 53s\n",
            "\tTrain Loss: 1.765 | Train Acc: 32.47%\n",
            "\t Val. Loss: 1.718 |  Val. Acc: 33.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:55<00:00,  5.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     769.737ms   11.847ms        0.000us    4.374ms   \n",
            "1      forward_pass       0.000us    0.000us         3.468s   11.081ms   \n",
            "2  loss_computation      25.967ms  191.575us        0.000us    5.202us   \n",
            "3  loss_computation       0.000us    0.000us       17.997ms   57.497us   \n",
            "4     backward_pass        3.945s   12.654ms        0.000us    1.503us   \n",
            "5     backward_pass       0.000us    0.000us      470.333us    1.503us   \n",
            "6    optimizer_step      17.086ms    2.964ms        0.000us  729.734us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216396735488>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -201751629824>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39700.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:07<00:00, 12.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Epoch Train Time: 1m 58s\n",
            "\tTrain Loss: 1.755 | Train Acc: 33.16%\n",
            "\t Val. Loss: 1.729 |  Val. Acc: 35.27%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:59<00:00,  5.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     788.343ms   12.295ms        0.000us    4.828ms   \n",
            "1      forward_pass       0.000us    0.000us         3.603s   11.513ms   \n",
            "2  loss_computation      27.157ms  199.526us        0.000us    5.846us   \n",
            "3  loss_computation       0.000us    0.000us       18.768ms   59.961us   \n",
            "4     backward_pass        4.009s   12.857ms        0.000us    1.716us   \n",
            "5     backward_pass       0.000us    0.000us      537.153us    1.716us   \n",
            "6    optimizer_step      17.921ms    3.148ms        0.000us  749.153us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216396866560>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -201752809472>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39680.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:08<00:00, 11.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Epoch Train Time: 2m 3s\n",
            "\tTrain Loss: 1.741 | Train Acc: 33.52%\n",
            "\t Val. Loss: 1.672 |  Val. Acc: 36.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:56<00:00,  5.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     771.731ms   11.866ms        0.000us    4.478ms   \n",
            "1      forward_pass       0.000us    0.000us         3.475s   11.101ms   \n",
            "2  loss_computation      26.176ms  192.501us        0.000us    5.366us   \n",
            "3  loss_computation       0.000us    0.000us       18.039ms   57.632us   \n",
            "4     backward_pass        3.969s   12.730ms        0.000us    1.551us   \n",
            "5     backward_pass       0.000us    0.000us      485.530us    1.551us   \n",
            "6    optimizer_step      17.205ms    2.970ms        0.000us  738.769us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216396735488>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -201751629824>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39700.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:07<00:00, 11.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Epoch Train Time: 1m 59s\n",
            "\tTrain Loss: 1.718 | Train Acc: 34.53%\n",
            "\t Val. Loss: 1.624 |  Val. Acc: 38.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:55<00:00,  5.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     766.669ms   11.876ms        0.000us    4.409ms   \n",
            "1      forward_pass       0.000us    0.000us         3.479s   11.115ms   \n",
            "2  loss_computation      26.067ms  192.040us        0.000us    5.186us   \n",
            "3  loss_computation       0.000us    0.000us       18.014ms   57.552us   \n",
            "4     backward_pass        3.955s   12.682ms        0.000us    1.517us   \n",
            "5     backward_pass       0.000us    0.000us      474.765us    1.517us   \n",
            "6    optimizer_step      16.908ms    3.039ms        0.000us  729.755us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                             0     216396866560>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -201752809472>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39680.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation: 100%|██████████| 94/94 [00:08<00:00, 11.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Epoch Train Time: 1m 58s\n",
            "\tTrain Loss: 1.713 | Train Acc: 34.63%\n",
            "\t Val. Loss: 1.639 |  Val. Acc: 37.57%\n",
            "Train finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "total_time = 0\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    train_loss, train_acc, start_time, end_time = train(model, train_iterator, criterion, optimizer, device, 7)\n",
        "\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
        "    #if valid_loss < best_valid_loss:\n",
        "        #best_valid_loss = valid_loss\n",
        "        #torch.save(model.state_dict(), 'vgg19-model.pt')\n",
        "\n",
        "    # end_time = time.monotonic()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    total_time += end_time - start_time\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Train Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc:.2f}%')\n",
        "\n",
        "print(\"Train finished\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "mddh-LmVYhVe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe78f2fe-5740-430a-86c0-a8f4be6e5c7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free memory: 39464.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        }
      ],
      "source": [
        "free_memory, total_memory = torch.cuda.mem_get_info()\n",
        "print(f\"Free memory: {free_memory / 1024**2:.2f} MB\")\n",
        "print(f\"Total memory: {total_memory / 1024**2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "0YfjvTjYWIR9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "795591a4-1050-46c4-f0b7-6e02e505a292"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet18 with dropback\n",
            "Total Training Time: 19m 28s\n"
          ]
        }
      ],
      "source": [
        "print(\"ResNet18 with dropback\")\n",
        "print(f'Total Training Time: {int(total_time/60)}m {int(total_time%60)}s')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GG41xbRBaqEt"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'trained_model.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPqa67aoatE7",
        "outputId": "61dd0989-b041-457f-8c3e-63415139c530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved model file size: 42.73 MB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "model_file_size = os.path.getsize('trained_model.pth')  # 바이트 단위\n",
        "model_file_size_MB = model_file_size / (1024 ** 2)  # MB로 변환\n",
        "print(f\"Saved model file size: {model_file_size_MB:.2f} MB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcaaiNYha3pK",
        "outputId": "ef18451a-0b63-41d1-9212-721345220c93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conv1.weight - Size: torch.Size([64, 3, 7, 7]) - Number of elements: 9408\n",
            "bn1.weight - Size: torch.Size([64]) - Number of elements: 64\n",
            "bn1.bias - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.0.conv1.weight - Size: torch.Size([64, 64, 3, 3]) - Number of elements: 36864\n",
            "layer1.0.bn1.weight - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.0.bn1.bias - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.0.conv2.weight - Size: torch.Size([64, 64, 3, 3]) - Number of elements: 36864\n",
            "layer1.0.bn2.weight - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.0.bn2.bias - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.1.conv1.weight - Size: torch.Size([64, 64, 3, 3]) - Number of elements: 36864\n",
            "layer1.1.bn1.weight - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.1.bn1.bias - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.1.conv2.weight - Size: torch.Size([64, 64, 3, 3]) - Number of elements: 36864\n",
            "layer1.1.bn2.weight - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer1.1.bn2.bias - Size: torch.Size([64]) - Number of elements: 64\n",
            "layer2.0.conv1.weight - Size: torch.Size([128, 64, 3, 3]) - Number of elements: 73728\n",
            "layer2.0.bn1.weight - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.0.bn1.bias - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.0.conv2.weight - Size: torch.Size([128, 128, 3, 3]) - Number of elements: 147456\n",
            "layer2.0.bn2.weight - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.0.bn2.bias - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.0.downsample.0.weight - Size: torch.Size([128, 64, 1, 1]) - Number of elements: 8192\n",
            "layer2.0.downsample.1.weight - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.0.downsample.1.bias - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.1.conv1.weight - Size: torch.Size([128, 128, 3, 3]) - Number of elements: 147456\n",
            "layer2.1.bn1.weight - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.1.bn1.bias - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.1.conv2.weight - Size: torch.Size([128, 128, 3, 3]) - Number of elements: 147456\n",
            "layer2.1.bn2.weight - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer2.1.bn2.bias - Size: torch.Size([128]) - Number of elements: 128\n",
            "layer3.0.conv1.weight - Size: torch.Size([256, 128, 3, 3]) - Number of elements: 294912\n",
            "layer3.0.bn1.weight - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.0.bn1.bias - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.0.conv2.weight - Size: torch.Size([256, 256, 3, 3]) - Number of elements: 589824\n",
            "layer3.0.bn2.weight - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.0.bn2.bias - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.0.downsample.0.weight - Size: torch.Size([256, 128, 1, 1]) - Number of elements: 32768\n",
            "layer3.0.downsample.1.weight - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.0.downsample.1.bias - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.1.conv1.weight - Size: torch.Size([256, 256, 3, 3]) - Number of elements: 589824\n",
            "layer3.1.bn1.weight - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.1.bn1.bias - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.1.conv2.weight - Size: torch.Size([256, 256, 3, 3]) - Number of elements: 589824\n",
            "layer3.1.bn2.weight - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer3.1.bn2.bias - Size: torch.Size([256]) - Number of elements: 256\n",
            "layer4.0.conv1.weight - Size: torch.Size([512, 256, 3, 3]) - Number of elements: 1179648\n",
            "layer4.0.bn1.weight - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.0.bn1.bias - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.0.conv2.weight - Size: torch.Size([512, 512, 3, 3]) - Number of elements: 2359296\n",
            "layer4.0.bn2.weight - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.0.bn2.bias - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.0.downsample.0.weight - Size: torch.Size([512, 256, 1, 1]) - Number of elements: 131072\n",
            "layer4.0.downsample.1.weight - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.0.downsample.1.bias - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.1.conv1.weight - Size: torch.Size([512, 512, 3, 3]) - Number of elements: 2359296\n",
            "layer4.1.bn1.weight - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.1.bn1.bias - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.1.conv2.weight - Size: torch.Size([512, 512, 3, 3]) - Number of elements: 2359296\n",
            "layer4.1.bn2.weight - Size: torch.Size([512]) - Number of elements: 512\n",
            "layer4.1.bn2.bias - Size: torch.Size([512]) - Number of elements: 512\n",
            "fc.weight - Size: torch.Size([10, 512]) - Number of elements: 5120\n",
            "fc.bias - Size: torch.Size([10]) - Number of elements: 10\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"{name} - Size: {param.size()} - Number of elements: {param.numel()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Biwb-ebya6TH",
        "outputId": "4cdc2011-1017-4e63-80a6-b5ca1e32320c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of parameters: 11181642\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"Total number of parameters: {total_params}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JEUocY4bDmO",
        "outputId": "209f5583-633a-4e8d-db67-0b71994b1846"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
            "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
            "              ReLU-3         [-1, 64, 112, 112]               0\n",
            "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
            "            Conv2d-5           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
            "              ReLU-7           [-1, 64, 56, 56]               0\n",
            "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
            "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
            "             ReLU-10           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-11           [-1, 64, 56, 56]               0\n",
            "           Conv2d-12           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-13           [-1, 64, 56, 56]             128\n",
            "             ReLU-14           [-1, 64, 56, 56]               0\n",
            "           Conv2d-15           [-1, 64, 56, 56]          36,864\n",
            "      BatchNorm2d-16           [-1, 64, 56, 56]             128\n",
            "             ReLU-17           [-1, 64, 56, 56]               0\n",
            "       BasicBlock-18           [-1, 64, 56, 56]               0\n",
            "           Conv2d-19          [-1, 128, 28, 28]          73,728\n",
            "      BatchNorm2d-20          [-1, 128, 28, 28]             256\n",
            "             ReLU-21          [-1, 128, 28, 28]               0\n",
            "           Conv2d-22          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-23          [-1, 128, 28, 28]             256\n",
            "           Conv2d-24          [-1, 128, 28, 28]           8,192\n",
            "      BatchNorm2d-25          [-1, 128, 28, 28]             256\n",
            "             ReLU-26          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-27          [-1, 128, 28, 28]               0\n",
            "           Conv2d-28          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-29          [-1, 128, 28, 28]             256\n",
            "             ReLU-30          [-1, 128, 28, 28]               0\n",
            "           Conv2d-31          [-1, 128, 28, 28]         147,456\n",
            "      BatchNorm2d-32          [-1, 128, 28, 28]             256\n",
            "             ReLU-33          [-1, 128, 28, 28]               0\n",
            "       BasicBlock-34          [-1, 128, 28, 28]               0\n",
            "           Conv2d-35          [-1, 256, 14, 14]         294,912\n",
            "      BatchNorm2d-36          [-1, 256, 14, 14]             512\n",
            "             ReLU-37          [-1, 256, 14, 14]               0\n",
            "           Conv2d-38          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-39          [-1, 256, 14, 14]             512\n",
            "           Conv2d-40          [-1, 256, 14, 14]          32,768\n",
            "      BatchNorm2d-41          [-1, 256, 14, 14]             512\n",
            "             ReLU-42          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-43          [-1, 256, 14, 14]               0\n",
            "           Conv2d-44          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-45          [-1, 256, 14, 14]             512\n",
            "             ReLU-46          [-1, 256, 14, 14]               0\n",
            "           Conv2d-47          [-1, 256, 14, 14]         589,824\n",
            "      BatchNorm2d-48          [-1, 256, 14, 14]             512\n",
            "             ReLU-49          [-1, 256, 14, 14]               0\n",
            "       BasicBlock-50          [-1, 256, 14, 14]               0\n",
            "           Conv2d-51            [-1, 512, 7, 7]       1,179,648\n",
            "      BatchNorm2d-52            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-53            [-1, 512, 7, 7]               0\n",
            "           Conv2d-54            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-55            [-1, 512, 7, 7]           1,024\n",
            "           Conv2d-56            [-1, 512, 7, 7]         131,072\n",
            "      BatchNorm2d-57            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-58            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-59            [-1, 512, 7, 7]               0\n",
            "           Conv2d-60            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-61            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-62            [-1, 512, 7, 7]               0\n",
            "           Conv2d-63            [-1, 512, 7, 7]       2,359,296\n",
            "      BatchNorm2d-64            [-1, 512, 7, 7]           1,024\n",
            "             ReLU-65            [-1, 512, 7, 7]               0\n",
            "       BasicBlock-66            [-1, 512, 7, 7]               0\n",
            "AdaptiveAvgPool2d-67            [-1, 512, 1, 1]               0\n",
            "           Linear-68                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 11,181,642\n",
            "Trainable params: 11,181,642\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 62.79\n",
            "Params size (MB): 42.65\n",
            "Estimated Total Size (MB): 106.01\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model, input_size=(3, 224, 224))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import profiler\n",
        "\n",
        "dummy_input = torch.randn(32, 3, 224, 224).cuda()\n",
        "\n",
        "# Profiling inference\n",
        "with profiler.profile(\n",
        "    activities=[\n",
        "       profiler.ProfilerActivity.CPU,\n",
        "        profiler.ProfilerActivity.CUDA,  # Include if using GPU\n",
        "    ],\n",
        "    on_trace_ready=profiler.tensorboard_trace_handler(\"./logs\"),  # Optional logging\n",
        "    record_shapes=True,\n",
        "    with_stack=True\n",
        ") as prof:\n",
        "    with torch.no_grad():\n",
        "        model(dummy_input)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\", row_limit=10))"
      ],
      "metadata": {
        "id": "2IcRxfQD5OUn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa4bf36e-2a85-427f-8ac1-8daea2d8f613"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::conv2d         1.57%     111.908us        60.56%       4.319ms     215.953us       0.000us         0.00%       2.693ms     134.658us            20  \n",
            "                                      aten::convolution         2.46%     175.189us        58.99%       4.207ms     210.358us       0.000us         0.00%       2.693ms     134.658us            20  \n",
            "                                     aten::_convolution         3.00%     214.062us        56.53%       4.032ms     201.598us       0.000us         0.00%       2.693ms     134.658us            20  \n",
            "                                aten::cudnn_convolution        36.04%       2.571ms        53.53%       3.818ms     190.895us       2.693ms        65.35%       2.693ms     134.658us            20  \n",
            "sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     652.093us        15.82%     652.093us      65.209us            10  \n",
            "void cudnn::engines_precompiled::nchwToNhwcKernel<fl...         0.00%       0.000us         0.00%       0.000us       0.000us     651.961us        15.82%     651.961us      17.157us            38  \n",
            "                                       aten::batch_norm         1.50%     106.943us        25.59%       1.825ms      91.257us       0.000us         0.00%     641.820us      32.091us            20  \n",
            "                           aten::_batch_norm_impl_index         1.51%     107.554us        24.09%       1.718ms      85.910us       0.000us         0.00%     641.820us      32.091us            20  \n",
            "                                 aten::cudnn_batch_norm        10.09%     719.965us        22.58%       1.611ms      80.532us     641.820us        15.57%     641.820us      32.091us            20  \n",
            "void cudnn::bn_fw_inf_1C11_kernel_NCHW<float, float,...         0.00%       0.000us         0.00%       0.000us       0.000us     641.820us        15.57%     641.820us      32.091us            20  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 7.132ms\n",
            "Self CUDA time total: 4.121ms\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CMzYjIBa5W2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}