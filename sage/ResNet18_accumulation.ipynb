{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0J0oT8gKcZp",
        "outputId": "5cac8ebb-b1ab-4ff1-e263-c74150853f6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pynvml\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynvml\n",
            "Successfully installed pynvml-11.5.3\n"
          ]
        }
      ],
      "source": [
        " !pip install pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6bWwANANKcZr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pynvml import *\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JKWlgSFaKcZr"
      },
      "outputs": [],
      "source": [
        "def print_gpu_utilization():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.cuda.current_device()  # 현재 GPU 디바이스 정보\n",
        "        allocated_memory = torch.cuda.memory_allocated(device) / 1024**3  # 메모리 사용량 (GB)\n",
        "        reserved_memory = torch.cuda.memory_reserved(device) / 1024**3  # 예약된 메모리 (GB)\n",
        "        print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
        "        print(f\"Reserved Memory: {reserved_memory:.2f} GB\")\n",
        "    else:\n",
        "        print(\"No GPU available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rEZUl5bqKcZs"
      },
      "outputs": [],
      "source": [
        "def print_summary(result):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fg8Tv5f6KcZs"
      },
      "outputs": [],
      "source": [
        "size = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OQ5lBp8JKcZs"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size, scale=(0.5, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMWkF_kgKcZs",
        "outputId": "e761d909-2a65-47ce-d2bf-54bdb20fdec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:02<00:00, 70.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-10\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FzdABEobKcZs"
      },
      "outputs": [],
      "source": [
        "VALID_RATIO = 0.7\n",
        "n_train_examples = int(len(trainset) * VALID_RATIO)\n",
        "n_valid_examples = len(trainset) - n_train_examples\n",
        "\n",
        "train_data, valid_data = data.random_split(trainset, [n_train_examples, n_valid_examples])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yHbtd5nFKcZs"
      },
      "outputs": [],
      "source": [
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transform = test_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP_jLBKNKcZs",
        "outputId": "61ebfe93-1048-4674-8234-341b7366f1f2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35000, 15000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(train_data), len(valid_data), len(testset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FmBdYmobKcZt"
      },
      "outputs": [],
      "source": [
        "sample_fraction = 0.2\n",
        "\n",
        "# 무작위 인덱스 생성\n",
        "train_indices = torch.randperm(len(trainset))[:int(len(trainset) * sample_fraction)]\n",
        "valid_indices = torch.randperm(len(valid_data))[:int(len(valid_data) * sample_fraction)]\n",
        "test_indices = torch.randperm(len(testset))[:int(len(testset) * sample_fraction)]\n",
        "\n",
        "# 서브셋 생성\n",
        "train_subset = Subset(trainset, train_indices)\n",
        "valid_subset = Subset(valid_data, valid_indices)\n",
        "test_subset = Subset(testset, test_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xPQkM89KcZt",
        "outputId": "c438eab4-c8b6-4077-c60c-df7f5084b2d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 3000, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(train_subset), len(valid_subset), len(test_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "itFIh8M4KcZt"
      },
      "outputs": [],
      "source": [
        "train_iterator = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "valid_iterator = DataLoader(valid_subset, batch_size=batch_size, shuffle=False)\n",
        "test_iterator = DataLoader(test_subset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MgUZwz62KcZt"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample = False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if downsample:\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            downsample = None\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        i = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            i = self.downsample(i)\n",
        "\n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YzO-kbk8KcZt"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, config, output_dim, zero_init_residual = False):\n",
        "        super().__init__()\n",
        "\n",
        "        block, n_blocks, channels = config\n",
        "        self.in_channels = channels[0]\n",
        "        assert len(n_blocks) == len(channels) == 4\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
        "        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride=2)\n",
        "        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride=2)\n",
        "        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(self.in_channels, output_dim)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "                #elif isinstance(m, Bottleneck):\n",
        "                    #nn.init.constant_(m.bn3.weight, 0)\n",
        "\n",
        "    def get_resnet_layer(self, block, n_blocks, channels, stride=1):\n",
        "        layers = []\n",
        "        if self.in_channels != block.expansion * channels:\n",
        "            downsample = True\n",
        "        else:\n",
        "            downsample = False\n",
        "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "        for i in range(1, n_blocks):\n",
        "            layers.append(block(block.expansion * channels, channels))\n",
        "\n",
        "        self.in_channels = block.expansion * channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        #x = self.layer1(x)\n",
        "        x = checkpoint.checkpoint(self.layer1, x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        h = x.view(x.shape[0], -1)\n",
        "        x = self.fc(h)\n",
        "        return x, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IXq_us1tKcZt"
      },
      "outputs": [],
      "source": [
        "ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ud4qMfNRKcZt"
      },
      "outputs": [],
      "source": [
        "resnet18_config = ResNetConfig(block = BasicBlock, n_blocks = [2, 2, 2, 2], channels = [64, 128, 256, 512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sSM3x5lTKcZt"
      },
      "outputs": [],
      "source": [
        "model = ResNet(resnet18_config, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HisCYbYLKcZt",
        "outputId": "5f95bc74-8da0-4ade-bfc2-36fb88c2ba87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rpyRirXLKcZt"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Gg4VWcdFKcZt"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "pretrained_model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qlJK2iGYKcZt"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim=True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "a42UwJBAKcZu"
      },
      "outputs": [],
      "source": [
        "pattern = re.compile(r'key=(?P<key>\\S+)\\s+'\n",
        "                     r'self_cpu_time=(?P<self_cpu_time>\\S+)\\s+'\n",
        "                     r'cpu_time=(?P<cpu_time>\\S+)\\s+'\n",
        "                     r'self_cuda_time=(?P<self_cuda_time>\\S+)\\s+'\n",
        "                     r'cuda_time=(?P<cuda_time>\\S+)\\s+'\n",
        "                     r'input_shapes=(?P<input_shapes>\\S*)\\s*'\n",
        "                     r'cpu_memory_usage=(?P<cpu_memory_usage>\\S*)\\s*'\n",
        "                     r'cuda_memory_usage=(?P<cuda_memory_usage>\\S*)')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.checkpoint as checkpoint\n",
        "\n",
        "def gradient_checkpoint(model, *inputs):\n",
        "\n",
        "  return model(*inputs)\n"
      ],
      "metadata": {
        "id": "6LwqTTQeO4wC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BZ6aVgWJKcZu"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device, micro_batch_size = 8):\n",
        "    start_time = time.monotonic()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with profile(\n",
        "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "        profile_memory=True,  # 메모리 사용량 추적\n",
        "        record_shapes=True  # 텐서 크기 기록\n",
        "    ) as prof:\n",
        "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # 기울기 초기화\n",
        "            memory_usage = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
        "\n",
        "            for i in range(0, len(inputs), micro_batch_size):\n",
        "                if i != 0:\n",
        "                    del micro_inputs, micro_labels\n",
        "                    torch.cuda.empty_cache()\n",
        "                micro_inputs = inputs[i:i+micro_batch_size]\n",
        "                micro_labels = labels[i:i+micro_batch_size]\n",
        "\n",
        "                with record_function(\"forward_pass\"):  # Forward pass 프로파일링\n",
        "                    outputs = model(micro_inputs)  # 모델 연산\n",
        "\n",
        "                with record_function(\"loss_computation\"):  # 손실 계산 프로파일링\n",
        "                    loss = criterion(outputs[0], micro_labels)  # 손실 계산\n",
        "\n",
        "                with record_function(\"backward_pass\"):  # Backward pass 프로파일링\n",
        "                    loss.backward()  # 역전파\n",
        "\n",
        "                with record_function(\"optimizer_step\"):\n",
        "                    optimizer.step()\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs[0], 1)\n",
        "                total += micro_labels.size(0)\n",
        "                correct += (predicted == micro_labels).sum().item()\n",
        "    end_time = time.monotonic()\n",
        "    selected_keys = [\"forward_pass\", \"loss_computation\", \"backward_pass\", \"optimizer_step\"]\n",
        "\n",
        "    # key_averages()로부터 얻은 평균값을 필터링\n",
        "    filtered_averages = [avg for avg in prof.key_averages() if avg.key in selected_keys]\n",
        "    extracted_data = []\n",
        "\n",
        "    for avg in filtered_averages:\n",
        "      avg_str = str(avg)\n",
        "      match = pattern.search(avg_str)\n",
        "      if match:\n",
        "        extracted_data.append(match.groupdict())\n",
        "    df = pd.DataFrame(extracted_data)\n",
        "    print(df)\n",
        "    free_memory, total_memory = torch.cuda.mem_get_info()\n",
        "    print(f\"Free memory: {free_memory / 1024**2:.2f} MB\")\n",
        "    print(f\"Total memory: {total_memory / 1024**2:.2f} MB\")\n",
        "    # 훈련 후 평균 손실과 정확도 계산\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, accuracy, start_time, end_time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs[0], labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs[0], 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    # print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    return epoch_loss, accuracy"
      ],
      "metadata": {
        "id": "0m_qg1biH1YO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ukqNnL_pH6jE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory_usage = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
        "print(memory_usage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXsSo7BJH8bZ",
        "outputId": "6f356f00-94e6-43a6-dc36-192e29fbbbe4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42.70654296875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "free_memory, total_memory = torch.cuda.mem_get_info()\n",
        "print(f\"Free memory: {free_memory / 1024**2:.2f} MB\")\n",
        "print(f\"Total memory: {total_memory / 1024**2:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyk55gAMH_ir",
        "outputId": "60133d19-1161-4643-e023-5cdaa3cf3ba1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free memory: 40026.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "total_time = 0\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    train_loss, train_acc, start_time, end_time = train(model, train_iterator, criterion, optimizer, device)\n",
        "\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
        "    #if valid_loss < best_valid_loss:\n",
        "        #best_valid_loss = valid_loss\n",
        "        #torch.save(model.state_dict(), 'vgg19-model.pt')\n",
        "\n",
        "    # end_time = time.monotonic()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    total_time += end_time - start_time\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Train Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc:.2f}%')\n",
        "\n",
        "print(\"Train finished\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b_5nuu4ILGb",
        "outputId": "96190ac5-e564-49c9-b1e4-c8b6f666867b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/313 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Training: 100%|██████████| 313/313 [00:57<00:00,  5.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass        2.353s   10.038ms        0.000us    1.834ms   \n",
            "1      forward_pass       0.000us    0.000us        11.509s    9.199ms   \n",
            "2  loss_computation     102.869ms  255.123us        0.000us    5.562us   \n",
            "3  loss_computation       0.000us    0.000us      100.985ms   80.788us   \n",
            "4     backward_pass       18.109s   14.547ms        0.000us    1.748us   \n",
            "5     backward_pass       0.000us    0.000us        2.185ms    1.748us   \n",
            "6    optimizer_step      58.581ms    2.670ms        0.000us  686.032us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       6340000     164948419072>  \n",
            "1                             0                0>  \n",
            "2                             0          1280512>  \n",
            "3                             0                0>  \n",
            "4                             0    -150266270208>  \n",
            "5                             0                0>  \n",
            "6                           248         94145536>  \n",
            "Free memory: 39460.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:04<00:00, 18.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Train Time: 2m 14s\n",
            "\tTrain Loss: 8.658 | Train Acc: 20.56%\n",
            "\t Val. Loss: 1.939 |  Val. Acc: 27.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:55<00:00,  5.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass        2.452s    9.621ms        0.000us    1.832ms   \n",
            "1      forward_pass       0.000us    0.000us        11.486s    9.189ms   \n",
            "2  loss_computation     109.839ms  197.347us        0.000us    5.597us   \n",
            "3  loss_computation       0.000us    0.000us       71.862ms   57.490us   \n",
            "4     backward_pass       17.575s   14.110ms        0.000us    1.746us   \n",
            "5     backward_pass       0.000us    0.000us        2.183ms    1.746us   \n",
            "6    optimizer_step      63.987ms    2.649ms        0.000us  680.875us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       6340000     164953383424>  \n",
            "1                             0                0>  \n",
            "2                             0          1280512>  \n",
            "3                             0                0>  \n",
            "4                             0    -150279704576>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39458.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 15.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Epoch Train Time: 2m 11s\n",
            "\tTrain Loss: 7.656 | Train Acc: 29.08%\n",
            "\t Val. Loss: 1.737 |  Val. Acc: 34.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:54<00:00,  5.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass        2.315s    9.204ms        0.000us    1.831ms   \n",
            "1      forward_pass       0.000us    0.000us        11.018s    8.815ms   \n",
            "2  loss_computation      95.462ms  174.486us        0.000us    5.535us   \n",
            "3  loss_computation       0.000us    0.000us       64.778ms   51.823us   \n",
            "4     backward_pass       16.547s   13.287ms        0.000us    1.747us   \n",
            "5     backward_pass       0.000us    0.000us        2.184ms    1.747us   \n",
            "6    optimizer_step      56.170ms    2.668ms        0.000us  674.860us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       6340000     164971782656>  \n",
            "1                             0                0>  \n",
            "2                             0          1280512>  \n",
            "3                             0                0>  \n",
            "4                             0    -150280131072>  \n",
            "5                             0                0>  \n",
            "6                             0         48956928>  \n",
            "Free memory: 39458.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 15.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Epoch Train Time: 2m 8s\n",
            "\tTrain Loss: 7.221 | Train Acc: 33.89%\n",
            "\t Val. Loss: 1.571 |  Val. Acc: 43.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:54<00:00,  5.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass        2.347s    9.299ms        0.000us    1.832ms   \n",
            "1      forward_pass       0.000us    0.000us        11.129s    8.903ms   \n",
            "2  loss_computation      96.366ms  177.876us        0.000us    5.578us   \n",
            "3  loss_computation       0.000us    0.000us       66.935ms   53.548us   \n",
            "4     backward_pass       16.600s   13.329ms        0.000us    1.745us   \n",
            "5     backward_pass       0.000us    0.000us        2.182ms    1.745us   \n",
            "6    optimizer_step      60.521ms    2.521ms        0.000us  675.341us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       6340000     164944667136>  \n",
            "1                             0                0>  \n",
            "2                             0          1280512>  \n",
            "3                             0                0>  \n",
            "4                             0    -150280131072>  \n",
            "5                             0                0>  \n",
            "6                             0         97913856>  \n",
            "Free memory: 39458.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 13.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Epoch Train Time: 2m 9s\n",
            "\tTrain Loss: 6.758 | Train Acc: 38.75%\n",
            "\t Val. Loss: 1.523 |  Val. Acc: 44.37%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [01:00<00:00,  5.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass        2.410s    9.585ms        0.000us    1.831ms   \n",
            "1      forward_pass       0.000us    0.000us        11.469s    9.175ms   \n",
            "2  loss_computation      99.085ms  183.918us        0.000us    5.595us   \n",
            "3  loss_computation       0.000us    0.000us       69.409ms   55.527us   \n",
            "4     backward_pass       16.802s   13.492ms        0.000us    1.745us   \n",
            "5     backward_pass       0.000us    0.000us        2.182ms    1.745us   \n",
            "6    optimizer_step      57.852ms    2.882ms        0.000us  675.608us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       6340000     164949795328>  \n",
            "1                             0                0>  \n",
            "2                             0          1280512>  \n",
            "3                             0                0>  \n",
            "4                             0    -150280130560>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39458.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 15.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Epoch Train Time: 2m 18s\n",
            "\tTrain Loss: 6.398 | Train Acc: 42.10%\n",
            "\t Val. Loss: 1.485 |  Val. Acc: 46.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [00:56<00:00,  5.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass        2.338s    9.353ms        0.000us    1.832ms   \n",
            "1      forward_pass       0.000us    0.000us        11.191s    8.953ms   \n",
            "2  loss_computation      97.036ms  180.279us        0.000us    5.545us   \n",
            "3  loss_computation       0.000us    0.000us       67.964ms   54.371us   \n",
            "4     backward_pass       16.714s   13.419ms        0.000us    1.746us   \n",
            "5     backward_pass       0.000us    0.000us        2.183ms    1.746us   \n",
            "6    optimizer_step      62.202ms    2.763ms        0.000us  675.685us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       6340000     164953809408>  \n",
            "1                             0                0>  \n",
            "2                             0          1280512>  \n",
            "3                             0                0>  \n",
            "4                             0    -150280131072>  \n",
            "5                             0                0>  \n",
            "6                             0         97520640>  \n",
            "Free memory: 39458.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:07<00:00, 12.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Epoch Train Time: 2m 10s\n",
            "\tTrain Loss: 6.021 | Train Acc: 46.09%\n",
            "\t Val. Loss: 1.428 |  Val. Acc: 48.87%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [01:03<00:00,  4.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass        2.410s    9.596ms        0.000us    1.832ms   \n",
            "1      forward_pass       0.000us    0.000us        11.479s    9.183ms   \n",
            "2  loss_computation      98.807ms  183.953us        0.000us    5.596us   \n",
            "3  loss_computation       0.000us    0.000us       68.950ms   55.160us   \n",
            "4     backward_pass       16.922s   13.587ms        0.000us    1.747us   \n",
            "5     backward_pass       0.000us    0.000us        2.183ms    1.747us   \n",
            "6    optimizer_step      58.987ms    2.870ms        0.000us  675.673us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       6340000     164968571392>  \n",
            "1                             0                0>  \n",
            "2                             0          1280512>  \n",
            "3                             0                0>  \n",
            "4                             0    -150280131072>  \n",
            "5                             0                0>  \n",
            "6                             0         48563712>  \n",
            "Free memory: 39458.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:07<00:00, 13.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Epoch Train Time: 2m 21s\n",
            "\tTrain Loss: 5.701 | Train Acc: 48.44%\n",
            "\t Val. Loss: 1.283 |  Val. Acc: 53.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [01:02<00:00,  5.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass        2.400s    9.548ms        0.000us    1.831ms   \n",
            "1      forward_pass       0.000us    0.000us        11.419s    9.135ms   \n",
            "2  loss_computation      99.430ms  184.921us        0.000us    5.598us   \n",
            "3  loss_computation       0.000us    0.000us       69.791ms   55.833us   \n",
            "4     backward_pass       16.951s   13.610ms        0.000us    1.747us   \n",
            "5     backward_pass       0.000us    0.000us        2.184ms    1.747us   \n",
            "6    optimizer_step      59.607ms    2.875ms        0.000us  675.806us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       6340000     164970979840>  \n",
            "1                             0                0>  \n",
            "2                             0          1280512>  \n",
            "3                             0                0>  \n",
            "4                             0    -150280131072>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39458.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:08<00:00, 10.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Epoch Train Time: 2m 23s\n",
            "\tTrain Loss: 5.362 | Train Acc: 52.13%\n",
            "\t Val. Loss: 1.245 |  Val. Acc: 53.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [01:06<00:00,  4.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass        2.407s    9.546ms        0.000us    1.832ms   \n",
            "1      forward_pass       0.000us    0.000us        11.420s    9.136ms   \n",
            "2  loss_computation      98.942ms  182.631us        0.000us    5.572us   \n",
            "3  loss_computation       0.000us    0.000us       68.220ms   54.576us   \n",
            "4     backward_pass       16.770s   13.465ms        0.000us    1.747us   \n",
            "5     backward_pass       0.000us    0.000us        2.184ms    1.747us   \n",
            "6    optimizer_step      58.370ms    2.853ms        0.000us  675.601us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       6340000     164953809408>  \n",
            "1                             0                0>  \n",
            "2                             0          1280512>  \n",
            "3                             0                0>  \n",
            "4                             0    -150280130048>  \n",
            "5                             0                0>  \n",
            "6                             0         48563712>  \n",
            "Free memory: 39458.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:07<00:00, 12.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Epoch Train Time: 2m 22s\n",
            "\tTrain Loss: 5.060 | Train Acc: 54.87%\n",
            "\t Val. Loss: 1.092 |  Val. Acc: 61.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 313/313 [01:03<00:00,  4.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass        2.428s    9.682ms        0.000us    1.832ms   \n",
            "1      forward_pass       0.000us    0.000us        11.580s    9.264ms   \n",
            "2  loss_computation     100.255ms  188.272us        0.000us    5.557us   \n",
            "3  loss_computation       0.000us    0.000us       71.378ms   57.102us   \n",
            "4     backward_pass       16.904s   13.572ms        0.000us    1.746us   \n",
            "5     backward_pass       0.000us    0.000us        2.182ms    1.746us   \n",
            "6    optimizer_step      58.511ms    3.108ms        0.000us  675.392us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       6340000     164947386880>  \n",
            "1                             0                0>  \n",
            "2                             0          1280512>  \n",
            "3                             0                0>  \n",
            "4                             0    -150280131072>  \n",
            "5                             0                0>  \n",
            "6                             0         48563712>  \n",
            "Free memory: 39458.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:09<00:00,  9.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Epoch Train Time: 2m 24s\n",
            "\tTrain Loss: 4.804 | Train Acc: 56.68%\n",
            "\t Val. Loss: 1.022 |  Val. Acc: 62.47%\n",
            "Train finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "memory_usage = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
        "print(memory_usage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8EpuUVGRJ0z",
        "outputId": "9d584d5c-344c-4928-ab9a-c0cc526a2494"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "211.7109375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import profiler\n",
        "\n",
        "dummy_input = torch.randn(32, 3, 224, 224).cuda()\n",
        "\n",
        "# Profiling inference\n",
        "with profiler.profile(\n",
        "    activities=[\n",
        "       profiler.ProfilerActivity.CPU,\n",
        "        profiler.ProfilerActivity.CUDA,  # Include if using GPU\n",
        "    ],\n",
        "    on_trace_ready=profiler.tensorboard_trace_handler(\"./logs\"),  # Optional logging\n",
        "    record_shapes=True,\n",
        "    with_stack=True\n",
        ") as prof:\n",
        "    with torch.no_grad():\n",
        "        model(dummy_input)\n",
        "\n",
        "\n",
        "# Print results\n",
        "print(prof.key_averages().table(sort_by=\"cuda_time_total\" if torch.cuda.is_available() else \"cpu_time_total\", row_limit=10))"
      ],
      "metadata": {
        "id": "4fpEvNGgyJGU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a735bafb-3886-4e77-b3d3-ac4e9c45bd16"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                           aten::conv2d         1.52%     154.564us        48.25%       4.902ms     245.084us       0.000us         0.00%       2.695ms     134.742us            20  \n",
            "                                      aten::convolution         2.07%     210.376us        46.73%       4.747ms     237.356us       0.000us         0.00%       2.695ms     134.742us            20  \n",
            "                                     aten::_convolution         2.40%     244.090us        44.66%       4.537ms     226.837us       0.000us         0.00%       2.695ms     134.742us            20  \n",
            "                                aten::cudnn_convolution        28.57%       2.902ms        42.26%       4.293ms     214.633us       2.695ms        65.46%       2.695ms     134.742us            20  \n",
            "                                     CheckpointFunction        18.35%       1.864ms        31.51%       3.201ms       3.201ms       0.000us         0.00%       1.085ms       1.085ms             1  \n",
            "sm80_xmma_fprop_implicit_gemm_tf32f32_tf32f32_f32_nh...         0.00%       0.000us         0.00%       0.000us       0.000us     654.333us        15.89%     654.333us      65.433us            10  \n",
            "void cudnn::engines_precompiled::nchwToNhwcKernel<fl...         0.00%       0.000us         0.00%       0.000us       0.000us     654.208us        15.89%     654.208us      17.216us            38  \n",
            "                                       aten::batch_norm         0.87%      88.368us        20.15%       2.047ms     102.330us       0.000us         0.00%     639.870us      31.994us            20  \n",
            "                           aten::_batch_norm_impl_index         1.28%     129.855us        19.28%       1.958ms      97.912us       0.000us         0.00%     639.870us      31.994us            20  \n",
            "                                 aten::cudnn_batch_norm         7.68%     780.531us        18.00%       1.828ms      91.419us     639.870us        15.54%     639.870us      31.994us            20  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 10.158ms\n",
            "Self CUDA time total: 4.117ms\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}