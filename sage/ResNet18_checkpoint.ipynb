{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0J0oT8gKcZp",
        "outputId": "bc53f111-53b9-4e38-be3d-2b7ffc2de396"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pynvml\n",
            "  Downloading pynvml-11.5.3-py3-none-any.whl.metadata (8.8 kB)\n",
            "Downloading pynvml-11.5.3-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pynvml\n",
            "Successfully installed pynvml-11.5.3\n"
          ]
        }
      ],
      "source": [
        " !pip install pynvml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "6bWwANANKcZr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import copy\n",
        "from collections import namedtuple\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "\n",
        "import cv2\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from PIL import Image\n",
        "\n",
        "from tqdm import tqdm\n",
        "from pynvml import *\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JKWlgSFaKcZr"
      },
      "outputs": [],
      "source": [
        "def print_gpu_utilization():\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.cuda.current_device()  # 현재 GPU 디바이스 정보\n",
        "        allocated_memory = torch.cuda.memory_allocated(device) / 1024**3  # 메모리 사용량 (GB)\n",
        "        reserved_memory = torch.cuda.memory_reserved(device) / 1024**3  # 예약된 메모리 (GB)\n",
        "        print(f\"Allocated Memory: {allocated_memory:.2f} GB\")\n",
        "        print(f\"Reserved Memory: {reserved_memory:.2f} GB\")\n",
        "    else:\n",
        "        print(\"No GPU available.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "rEZUl5bqKcZs"
      },
      "outputs": [],
      "source": [
        "def print_summary(result):\n",
        "    print(f\"Time: {result.metrics['train_runtime']:.2f}\")\n",
        "    print(f\"Samples/second: {result.metrics['train_samples_per_second']:.2f}\")\n",
        "    print_gpu_utilization()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fg8Tv5f6KcZs"
      },
      "outputs": [],
      "source": [
        "size = 224\n",
        "mean = (0.485, 0.456, 0.406)\n",
        "std = (0.229, 0.224, 0.225)\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "OQ5lBp8JKcZs"
      },
      "outputs": [],
      "source": [
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(size, scale=(0.5, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uMWkF_kgKcZs",
        "outputId": "cb9f79d5-154f-4a9c-d323-6068637e95dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:13<00:00, 12.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# CIFAR-10\n",
        "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transforms)\n",
        "trainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transforms)\n",
        "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FzdABEobKcZs"
      },
      "outputs": [],
      "source": [
        "VALID_RATIO = 0.7\n",
        "n_train_examples = int(len(trainset) * VALID_RATIO)\n",
        "n_valid_examples = len(trainset) - n_train_examples\n",
        "\n",
        "train_data, valid_data = data.random_split(trainset, [n_train_examples, n_valid_examples])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yHbtd5nFKcZs"
      },
      "outputs": [],
      "source": [
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transform = test_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vP_jLBKNKcZs",
        "outputId": "a2c6dcdf-6dec-41e9-c7c2-3a96833d11bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(35000, 15000, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "len(train_data), len(valid_data), len(testset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FmBdYmobKcZt"
      },
      "outputs": [],
      "source": [
        "sample_fraction = 0.2\n",
        "\n",
        "# 무작위 인덱스 생성\n",
        "train_indices = torch.randperm(len(trainset))[:int(len(trainset) * sample_fraction)]\n",
        "valid_indices = torch.randperm(len(valid_data))[:int(len(valid_data) * sample_fraction)]\n",
        "test_indices = torch.randperm(len(testset))[:int(len(testset) * sample_fraction)]\n",
        "\n",
        "# 서브셋 생성\n",
        "train_subset = Subset(trainset, train_indices)\n",
        "valid_subset = Subset(valid_data, valid_indices)\n",
        "test_subset = Subset(testset, test_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xPQkM89KcZt",
        "outputId": "993fe05d-7acd-4901-ea3c-8be7bdb7e6ee"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 3000, 2000)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(train_subset), len(valid_subset), len(test_subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "itFIh8M4KcZt"
      },
      "outputs": [],
      "source": [
        "train_iterator = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "valid_iterator = DataLoader(valid_subset, batch_size=batch_size, shuffle=False)\n",
        "test_iterator = DataLoader(test_subset, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "MgUZwz62KcZt"
      },
      "outputs": [],
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample = False):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if downsample:\n",
        "            conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False)\n",
        "            bn = nn.BatchNorm2d(out_channels)\n",
        "            downsample = nn.Sequential(conv, bn)\n",
        "        else:\n",
        "            downsample = None\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        i = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            i = self.downsample(i)\n",
        "\n",
        "        x += i\n",
        "        x = self.relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YzO-kbk8KcZt"
      },
      "outputs": [],
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, config, output_dim, zero_init_residual = False):\n",
        "        super().__init__()\n",
        "\n",
        "        block, n_blocks, channels = config\n",
        "        self.in_channels = channels[0]\n",
        "        assert len(n_blocks) == len(channels) == 4\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, self.in_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        self.layer1 = self.get_resnet_layer(block, n_blocks[0], channels[0])\n",
        "        self.layer2 = self.get_resnet_layer(block, n_blocks[1], channels[1], stride=2)\n",
        "        self.layer3 = self.get_resnet_layer(block, n_blocks[2], channels[2], stride=2)\n",
        "        self.layer4 = self.get_resnet_layer(block, n_blocks[3], channels[3], stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(self.in_channels, output_dim)\n",
        "\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "                #elif isinstance(m, Bottleneck):\n",
        "                    #nn.init.constant_(m.bn3.weight, 0)\n",
        "\n",
        "    def get_resnet_layer(self, block, n_blocks, channels, stride=1):\n",
        "        layers = []\n",
        "        if self.in_channels != block.expansion * channels:\n",
        "            downsample = True\n",
        "        else:\n",
        "            downsample = False\n",
        "        layers.append(block(self.in_channels, channels, stride, downsample))\n",
        "        for i in range(1, n_blocks):\n",
        "            layers.append(block(block.expansion * channels, channels))\n",
        "\n",
        "        self.in_channels = block.expansion * channels\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        #x = self.layer1(x)\n",
        "        x = checkpoint.checkpoint(self.layer1, x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        h = x.view(x.shape[0], -1)\n",
        "        x = self.fc(h)\n",
        "        return x, h"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IXq_us1tKcZt"
      },
      "outputs": [],
      "source": [
        "ResNetConfig = namedtuple('ResNetConfig', ['block', 'n_blocks', 'channels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ud4qMfNRKcZt"
      },
      "outputs": [],
      "source": [
        "resnet18_config = ResNetConfig(block = BasicBlock, n_blocks = [2, 2, 2, 2], channels = [64, 128, 256, 512])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "sSM3x5lTKcZt"
      },
      "outputs": [],
      "source": [
        "model = ResNet(resnet18_config, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HisCYbYLKcZt",
        "outputId": "c4d63f23-c8fb-4481-bf1b-ab7f98a275be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rpyRirXLKcZt"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Gg4VWcdFKcZt"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "pretrained_model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "qlJK2iGYKcZt"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim=True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "a42UwJBAKcZu"
      },
      "outputs": [],
      "source": [
        "pattern = re.compile(r'key=(?P<key>\\S+)\\s+'\n",
        "                     r'self_cpu_time=(?P<self_cpu_time>\\S+)\\s+'\n",
        "                     r'cpu_time=(?P<cpu_time>\\S+)\\s+'\n",
        "                     r'self_cuda_time=(?P<self_cuda_time>\\S+)\\s+'\n",
        "                     r'cuda_time=(?P<cuda_time>\\S+)\\s+'\n",
        "                     r'input_shapes=(?P<input_shapes>\\S*)\\s*'\n",
        "                     r'cpu_memory_usage=(?P<cpu_memory_usage>\\S*)\\s*'\n",
        "                     r'cuda_memory_usage=(?P<cuda_memory_usage>\\S*)')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.utils.checkpoint as checkpoint\n",
        "\n",
        "def gradient_checkpoint(model, *inputs):\n",
        "\n",
        "  return model(*inputs)\n"
      ],
      "metadata": {
        "id": "6LwqTTQeO4wC"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BZ6aVgWJKcZu"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, criterion, optimizer, device):\n",
        "    start_time = time.monotonic()\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with profile(\n",
        "        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],\n",
        "        profile_memory=True,  # 메모리 사용량 추적\n",
        "        record_shapes=True  # 텐서 크기 기록\n",
        "    ) as prof:\n",
        "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()  # 기울기 초기화\n",
        "            memory_usage = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
        "            #print(f\"Memory usage: {memory_usage:.2f} MB\")\n",
        "            with record_function(\"forward_pass\"):\n",
        "              outputs = model(inputs)\n",
        "            del inputs\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            with record_function(\"loss_computation\"):\n",
        "                loss = criterion(outputs[0], labels)\n",
        "            with record_function(\"backward_pass\"):\n",
        "                loss.backward()\n",
        "            with record_function(\"optimizer_step\"):\n",
        "                optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs[0], 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    end_time = time.monotonic()\n",
        "    selected_keys = [\"forward_pass\", \"loss_computation\", \"backward_pass\", \"optimizer_step\"]\n",
        "\n",
        "    # key_averages()로부터 얻은 평균값을 필터링\n",
        "    filtered_averages = [avg for avg in prof.key_averages() if avg.key in selected_keys]\n",
        "    extracted_data = []\n",
        "\n",
        "    for avg in filtered_averages:\n",
        "      avg_str = str(avg)\n",
        "      match = pattern.search(avg_str)\n",
        "      if match:\n",
        "        extracted_data.append(match.groupdict())\n",
        "    df = pd.DataFrame(extracted_data)\n",
        "    print(df)\n",
        "    free_memory, total_memory = torch.cuda.mem_get_info()\n",
        "    print(f\"Free memory: {free_memory / 1024**2:.2f} MB\")\n",
        "    print(f\"Total memory: {total_memory / 1024**2:.2f} MB\")\n",
        "    # 훈련 후 평균 손실과 정확도 계산\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "\n",
        "    return epoch_loss, accuracy, start_time, end_time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, data_loader, criterion, device, phase=\"Validation\"):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc=f\"{phase}\"):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs[0], labels)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs[0], 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    epoch_loss = running_loss / len(data_loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    # print(f\"{phase} Loss: {epoch_loss:.4f}, {phase} Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    return epoch_loss, accuracy"
      ],
      "metadata": {
        "id": "0m_qg1biH1YO"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "ukqNnL_pH6jE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory_usage = torch.cuda.memory_allocated(device) / (1024 ** 2)\n",
        "print(memory_usage)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXsSo7BJH8bZ",
        "outputId": "4bdd6a89-b71e-4efa-cecc-9a3a9adba36c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42.70654296875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "free_memory, total_memory = torch.cuda.mem_get_info()\n",
        "print(f\"Free memory: {free_memory / 1024**2:.2f} MB\")\n",
        "print(f\"Total memory: {total_memory / 1024**2:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyk55gAMH_ir",
        "outputId": "1fb5b180-b766-4233-cb15-760a838ea1b3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free memory: 40026.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "total_time = 0\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    train_loss, train_acc, start_time, end_time = train(model, train_iterator, criterion, optimizer, device)\n",
        "\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
        "    #if valid_loss < best_valid_loss:\n",
        "        #best_valid_loss = valid_loss\n",
        "        #torch.save(model.state_dict(), 'vgg19-model.pt')\n",
        "\n",
        "    # end_time = time.monotonic()\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    total_time += end_time - start_time\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Train Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc:.2f}%')\n",
        "\n",
        "print(\"Train finished\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4b_5nuu4ILGb",
        "outputId": "5a8f039b-c338-4d3c-f5f2-af74bc2e0741"
      },
      "execution_count": 30,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   0%|          | 0/313 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "Training: 100%|██████████| 313/313 [00:31<00:00, 10.06it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     567.399ms   11.854ms        0.000us    4.682ms   \n",
            "1      forward_pass       0.000us    0.000us         3.040s    9.682ms   \n",
            "2  loss_computation      35.415ms  524.609us        0.000us    6.429us   \n",
            "3  loss_computation       0.000us    0.000us       51.121ms  163.327us   \n",
            "4     backward_pass        5.541s   17.812ms        0.000us    1.628us   \n",
            "5     backward_pass       0.000us    0.000us      509.557us    1.628us   \n",
            "6    optimizer_step      14.479ms    3.438ms        0.000us  699.914us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       1587536     162229720064>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -153761382400>  \n",
            "5                             0                0>  \n",
            "6                           248         91294720>  \n",
            "Free memory: 39160.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:04<00:00, 19.74it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Epoch Train Time: 1m 4s\n",
            "\tTrain Loss: 1.999 | Train Acc: 25.18%\n",
            "\t Val. Loss: 1.836 |  Val. Acc: 31.97%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [00:29<00:00, 10.75it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     580.760ms    8.518ms        0.000us    4.685ms   \n",
            "1      forward_pass       0.000us    0.000us         2.579s    8.240ms   \n",
            "2  loss_computation      35.282ms  243.577us        0.000us    7.065us   \n",
            "3  loss_computation       0.000us    0.000us       19.101ms   61.026us   \n",
            "4     backward_pass        5.085s   16.298ms        0.000us    1.641us   \n",
            "5     backward_pass       0.000us    0.000us      513.507us    1.641us   \n",
            "6    optimizer_step      14.621ms    2.984ms        0.000us  699.229us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       1587536     161009734656>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -152701632512>  \n",
            "5                             0                0>  \n",
            "6                             0         47220224>  \n",
            "Free memory: 39188.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:05<00:00, 17.50it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 02 | Epoch Train Time: 1m 1s\n",
            "\tTrain Loss: 1.787 | Train Acc: 33.18%\n",
            "\t Val. Loss: 1.592 |  Val. Acc: 40.77%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [00:27<00:00, 11.40it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     590.568ms    8.284ms        0.000us    4.421ms   \n",
            "1      forward_pass       0.000us    0.000us         2.525s    8.066ms   \n",
            "2  loss_computation      31.284ms  226.212us        0.000us    6.627us   \n",
            "3  loss_computation       0.000us    0.000us       18.249ms   58.302us   \n",
            "4     backward_pass        4.812s   15.426ms        0.000us    1.513us   \n",
            "5     backward_pass       0.000us    0.000us      473.631us    1.513us   \n",
            "6    optimizer_step      14.791ms    3.004ms        0.000us  692.878us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       1587536     160334255104>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -151925161984>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39188.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:05<00:00, 17.55it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 03 | Epoch Train Time: 0m 59s\n",
            "\tTrain Loss: 1.656 | Train Acc: 38.93%\n",
            "\t Val. Loss: 1.504 |  Val. Acc: 44.80%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [00:27<00:00, 11.57it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     592.210ms    8.277ms        0.000us    4.332ms   \n",
            "1      forward_pass       0.000us    0.000us         2.522s    8.059ms   \n",
            "2  loss_computation      31.492ms  227.290us        0.000us    6.482us   \n",
            "3  loss_computation       0.000us    0.000us       18.083ms   57.774us   \n",
            "4     backward_pass        4.844s   15.527ms        0.000us    1.473us   \n",
            "5     backward_pass       0.000us    0.000us      461.179us    1.473us   \n",
            "6    optimizer_step      14.154ms    2.876ms        0.000us  690.810us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       1587536     160334255104>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -151925161984>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39188.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 15.05it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 04 | Epoch Train Time: 0m 59s\n",
            "\tTrain Loss: 1.549 | Train Acc: 43.74%\n",
            "\t Val. Loss: 1.466 |  Val. Acc: 45.97%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [00:32<00:00,  9.60it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     572.382ms    8.438ms        0.000us    4.888ms   \n",
            "1      forward_pass       0.000us    0.000us         2.568s    8.204ms   \n",
            "2  loss_computation      32.389ms  235.107us        0.000us    6.528us   \n",
            "3  loss_computation       0.000us    0.000us       19.534ms   62.409us   \n",
            "4     backward_pass        4.866s   15.601ms        0.000us    1.714us   \n",
            "5     backward_pass       0.000us    0.000us      536.509us    1.714us   \n",
            "6    optimizer_step      14.976ms    3.145ms        0.000us  700.231us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       1587536     160334255104>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -151925161984>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39188.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 14.32it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 05 | Epoch Train Time: 1m 4s\n",
            "\tTrain Loss: 1.413 | Train Acc: 48.61%\n",
            "\t Val. Loss: 1.237 |  Val. Acc: 55.30%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [00:33<00:00,  9.27it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     576.176ms    8.571ms        0.000us    4.885ms   \n",
            "1      forward_pass       0.000us    0.000us         2.609s    8.334ms   \n",
            "2  loss_computation      32.684ms  235.449us        0.000us    6.523us   \n",
            "3  loss_computation       0.000us    0.000us       19.559ms   62.489us   \n",
            "4     backward_pass        4.862s   15.586ms        0.000us    1.714us   \n",
            "5     backward_pass       0.000us    0.000us      536.456us    1.714us   \n",
            "6    optimizer_step      14.377ms    3.136ms        0.000us  699.896us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       1587536     160334255104>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -151925161984>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39188.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 14.72it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 06 | Epoch Train Time: 1m 6s\n",
            "\tTrain Loss: 1.294 | Train Acc: 53.73%\n",
            "\t Val. Loss: 1.224 |  Val. Acc: 55.73%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [00:30<00:00, 10.18it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     561.776ms    8.269ms        0.000us    4.863ms   \n",
            "1      forward_pass       0.000us    0.000us         2.516s    8.039ms   \n",
            "2  loss_computation      32.006ms  232.914us        0.000us    6.490us   \n",
            "3  loss_computation       0.000us    0.000us       19.602ms   62.628us   \n",
            "4     backward_pass        4.833s   15.495ms        0.000us    1.704us   \n",
            "5     backward_pass       0.000us    0.000us      533.446us    1.704us   \n",
            "6    optimizer_step      14.450ms    2.910ms        0.000us  699.149us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       1587536     160334255104>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -151925161984>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39188.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 14.97it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 07 | Epoch Train Time: 1m 2s\n",
            "\tTrain Loss: 1.200 | Train Acc: 56.95%\n",
            "\t Val. Loss: 1.084 |  Val. Acc: 61.27%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [00:34<00:00,  9.19it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     574.155ms    8.471ms        0.000us    4.886ms   \n",
            "1      forward_pass       0.000us    0.000us         2.578s    8.235ms   \n",
            "2  loss_computation      32.747ms  237.862us        0.000us    6.523us   \n",
            "3  loss_computation       0.000us    0.000us       19.924ms   63.657us   \n",
            "4     backward_pass        4.901s   15.708ms        0.000us    1.715us   \n",
            "5     backward_pass       0.000us    0.000us      536.725us    1.715us   \n",
            "6    optimizer_step      14.465ms    3.213ms        0.000us  699.742us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       1587536     160334255104>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -151925161984>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39188.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:05<00:00, 15.95it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 08 | Epoch Train Time: 1m 7s\n",
            "\tTrain Loss: 1.142 | Train Acc: 59.00%\n",
            "\t Val. Loss: 0.959 |  Val. Acc: 66.57%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [00:29<00:00, 10.58it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     563.600ms    8.257ms        0.000us    4.809ms   \n",
            "1      forward_pass       0.000us    0.000us         2.513s    8.027ms   \n",
            "2  loss_computation      32.420ms  232.257us        0.000us    6.396us   \n",
            "3  loss_computation       0.000us    0.000us       19.367ms   61.874us   \n",
            "4     backward_pass        4.856s   15.570ms        0.000us    1.681us   \n",
            "5     backward_pass       0.000us    0.000us      526.259us    1.681us   \n",
            "6    optimizer_step      14.734ms    3.042ms        0.000us  698.714us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       1587536     160334255104>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -151925161984>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39188.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:07<00:00, 12.76it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 09 | Epoch Train Time: 1m 1s\n",
            "\tTrain Loss: 1.062 | Train Acc: 62.32%\n",
            "\t Val. Loss: 0.911 |  Val. Acc: 68.60%\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 313/313 [00:36<00:00,  8.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                key self_cpu_time   cpu_time self_cuda_time  cuda_time  \\\n",
            "0      forward_pass     583.334ms    8.535ms        0.000us    4.885ms   \n",
            "1      forward_pass       0.000us    0.000us         2.595s    8.292ms   \n",
            "2  loss_computation      32.711ms  237.030us        0.000us    6.522us   \n",
            "3  loss_computation       0.000us    0.000us       19.827ms   63.346us   \n",
            "4     backward_pass        4.911s   15.744ms        0.000us    1.713us   \n",
            "5     backward_pass       0.000us    0.000us      536.148us    1.713us   \n",
            "6    optimizer_step      14.646ms    3.202ms        0.000us  700.161us   \n",
            "\n",
            "  input_shapes cpu_memory_usage cuda_memory_usage  \n",
            "0                       1587536     160334255104>  \n",
            "1                             0                0>  \n",
            "2                             0           641024>  \n",
            "3                             0                0>  \n",
            "4                             0    -151925161984>  \n",
            "5                             0                0>  \n",
            "6                             0                0>  \n",
            "Free memory: 39188.81 MB\n",
            "Total memory: 40513.81 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Validation:   0%|          | 0/94 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_dynamo/eval_frame.py:632: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
            "  warnings.warn(\n",
            "Validation: 100%|██████████| 94/94 [00:06<00:00, 14.33it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Epoch Train Time: 1m 8s\n",
            "\tTrain Loss: 1.013 | Train Acc: 63.97%\n",
            "\t Val. Loss: 0.943 |  Val. Acc: 68.27%\n",
            "Train finished\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}